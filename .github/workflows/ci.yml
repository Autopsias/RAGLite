# RAGLite CI/CD Pipeline
# Optimized with UV package manager for 10-100x faster installs
# Supports self-hosted macOS runners with parallel execution
# Based on Powerpoint Agent Generator CI best practices
# Updated: 2025-10-28 - Test Suite Consolidation
# Updated: 2025-10-29 - Runner Isolation (raglite label) + Resource Limits
#
# IMPORTANT: All tests consolidated in tests/ directory (2025-10-28)
# Total test count: ~402 tests (200+ unit, 115+ integration, 28 e2e)
# NFR validation: Accuracy (NFR6/NFR7), Performance (NFR13), Coverage (80%)
#
# RUNNER ISOLATION (2025-10-29):
# - Uses ONLY runners with 'raglite' label (raglite-runner-1, raglite-runner-2)
# - Prevents cross-project resource contention (20 total system runners)
# - Pytest workers limited to 4 (prevents 110-process explosion)
# - Expected: 2 jobs max parallel √ó 4 workers = 8 processes (~3-5 GB RAM)

name: CI

on:
  push:
    branches: [main, develop, "story/**", "epic/**"]
  pull_request:
    branches: [main, develop]
  workflow_dispatch:

# Restrict permissions (security best practice)
permissions:
  contents: read
  pull-requests: write
  checks: write

# Cancel in-progress runs for same branch/PR
concurrency:
  group: ci-${{ github.ref }}
  cancel-in-progress: true

jobs:
  # ============================================================================
  # CODE QUALITY CHECKS
  # ============================================================================

  # ============================================================================
  # JOB 1: Linting & Formatting
  # ============================================================================
  lint:
    name: "üìù Code Quality: Lint & Format"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install ruff black isort

      - name: Run Ruff linter
        run: |
          source ci-venv/bin/activate
          ruff check . --output-format=github
        continue-on-error: true

      - name: Run Black formatter check
        run: |
          source ci-venv/bin/activate
          black --check --diff raglite/ tests/ scripts/
        continue-on-error: true

      - name: Run isort import check
        run: |
          source ci-venv/bin/activate
          isort --check-only --diff raglite/ tests/ scripts/
        continue-on-error: true

  # ============================================================================
  # JOB 2: Type Checking
  # ============================================================================
  type-check:
    name: "üîç Code Quality: Type Check (mypy)"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install -e .
          uv pip install mypy types-requests

      - name: Run mypy on raglite/ package
        run: |
          source ci-venv/bin/activate
          mypy raglite/ --show-error-codes --ignore-missing-imports
        continue-on-error: true

  # ============================================================================
  # JOB 3: Security Scanning
  # ============================================================================
  security:
    name: "üîí Code Quality: Security Scan"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install security scanning tools with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install bandit[toml] safety
          uv pip install -e .

      - name: Run Bandit security linter
        run: |
          source ci-venv/bin/activate
          bandit -r raglite/ -f json -o bandit-report.json || true
        continue-on-error: true

      - name: Run Safety dependency scanner
        run: |
          source ci-venv/bin/activate
          safety check --json || true
        continue-on-error: true

      - name: Upload Bandit report
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: bandit-security-report
          path: bandit-report.json
          retention-days: 30

  # ============================================================================
  # TEST SUITES
  # ============================================================================

  # ============================================================================
  # JOB 4: Unit Tests (~200 tests)
  # ============================================================================
  test-unit:
    name: "üß™ Tests: Unit (~200 tests)"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 10

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install -e .
          uv pip install pytest pytest-cov pytest-asyncio pytest-xdist pytest-timeout

      - name: Run unit tests with coverage
        run: |
          source ci-venv/bin/activate
          echo "========================================="
          echo "CI MODE: Unit Tests with Coverage"
          echo "- Running ALL unit tests (including slow)"
          echo "- Parallel workers: 4 (limited for stability)"
          echo "- Target: <10 minutes"
          echo "========================================="
          pytest tests/unit/ \
            -n 4 \
            --dist worksteal \
            -m "" \
            --cov=raglite \
            --cov-report=xml \
            --cov-report=term-missing \
            --cov-report=html \
            --junitxml=pytest-unit-report.xml \
            -v

      - name: Enforce coverage threshold (80%)
        run: |
          source ci-venv/bin/activate
          coverage report --fail-under=80
        continue-on-error: true

      - name: Upload unit test coverage
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-unit
          path: |
            coverage.xml
            htmlcov/
            pytest-unit-report.xml
          retention-days: 30

  # ============================================================================
  # JOB 5: Integration Tests (~115 tests, requires Qdrant)
  # CI MODE: Uses 160-page full PDF for comprehensive testing
  # ============================================================================
  test-integration:
    name: "üîó Tests: Integration (~115 tests)"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 60
    needs: [test-unit]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install -e .
          uv pip install pytest pytest-asyncio pytest-xdist pytest-timeout

      - name: Verify Qdrant is available
        run: |
          echo "Checking persistent Qdrant instance..."

          # Wait for Qdrant to be ready (max 30 seconds)
          MAX_RETRIES=30
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -sf http://localhost:6333/collections > /dev/null 2>&1; then
              echo "‚úÖ Qdrant is available at localhost:6333"

              # Additional health check: verify cluster status
              CLUSTER_STATUS=$(curl -sf http://localhost:6333/cluster | python3 -c "import sys, json; print(json.load(sys.stdin)['status'])" 2>/dev/null || echo "unknown")
              echo "Cluster status: $CLUSTER_STATUS"

              # Check telemetry for any recent 500 errors
              ERROR_500_COUNT=$(curl -sf http://localhost:6333/telemetry 2>/dev/null | python3 -c "import sys, json; data=json.load(sys.stdin); print(sum(endpoint.get('500', {}).get('count', 0) for endpoint in data['result']['requests']['rest']['responses'].values()))" 2>/dev/null || echo "0")
              echo "Recent 500 errors: $ERROR_500_COUNT"

              if [ "$ERROR_500_COUNT" -gt 0 ]; then
                echo "‚ö†Ô∏è  Warning: Qdrant has $ERROR_500_COUNT recent 500 errors"
                echo "Restarting Qdrant to clear error state..."
                docker restart raglite-qdrant
                sleep 5
              fi

              exit 0
            fi

            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "Waiting for Qdrant... (attempt $RETRY_COUNT/$MAX_RETRIES)"
            sleep 1
          done

          echo "‚ùå Qdrant not available after ${MAX_RETRIES}s"
          echo "To start: docker run -d -p 6333:6333 --name raglite-qdrant qdrant/qdrant:v1.15.0"
          exit 1

      - name: Run integration tests (CI comprehensive mode)
        env:
          QDRANT_HOST: localhost
          QDRANT_PORT: 6333
          TEST_USE_FULL_PDF: "true"
        run: |
          source ci-venv/bin/activate
          echo "========================================="
          echo "CI MODE: Comprehensive Testing"
          echo "- Using 160-page full PDF (~150s ingestion)"
          echo "- Running ALL tests including slow ones"
          echo "- Target: 30-50 minutes total"
          echo "========================================="
          if [ -d "tests/integration" ] && [ -n "$(find tests/integration -name 'test_*.py' -type f)" ]; then
            pytest tests/integration/ \
              -v \
              -m "" \
              --junitxml=pytest-integration-report.xml
          else
            echo "‚ö†Ô∏è  No integration tests found, skipping..."
          fi
        continue-on-error: true

      - name: Upload integration test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: integration-test-results
          path: pytest-integration-report.xml
          retention-days: 30

  # ============================================================================
  # JOB 6: E2E Tests (~28 tests)
  # ============================================================================
  test-e2e:
    name: "üéØ Tests: E2E (~28 tests)"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 15
    needs: [test-unit]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install -e .
          uv pip install pytest pytest-asyncio pytest-xdist pytest-timeout

      - name: Run E2E tests
        run: |
          source ci-venv/bin/activate
          echo "========================================="
          echo "CI MODE: E2E Tests"
          echo "- Running ALL E2E tests (including slow)"
          echo "- Target: <15 minutes"
          echo "========================================="
          if [ -d "tests/e2e" ] && [ -n "$(find tests/e2e -name 'test_*.py' -type f)" ]; then
            pytest tests/e2e/ \
              -v \
              -m "" \
              --junitxml=pytest-e2e-report.xml
          else
            echo "‚ö†Ô∏è  No E2E tests found, skipping..."
          fi

      - name: Upload E2E test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: e2e-test-results
          path: pytest-e2e-report.xml
          retention-days: 30

  # ============================================================================
  # NFR VALIDATION
  # ============================================================================

  # ============================================================================
  # JOB 7: Ground Truth Accuracy Validation (NFR6/NFR7)
  # ============================================================================
  test-accuracy:
    name: "‚úÖ NFR: Accuracy Validation (NFR6/NFR7)"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 30
    needs: [test-integration]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install -e .
          uv pip install pytest pytest-asyncio

      - name: Verify Qdrant is available
        run: |
          echo "Checking persistent Qdrant instance..."

          # Wait for Qdrant to be ready (max 30 seconds)
          MAX_RETRIES=30
          RETRY_COUNT=0

          while [ $RETRY_COUNT -lt $MAX_RETRIES ]; do
            if curl -sf http://localhost:6333/collections > /dev/null 2>&1; then
              echo "‚úÖ Qdrant available for accuracy tests"

              # Check telemetry for any recent 500 errors
              ERROR_500_COUNT=$(curl -sf http://localhost:6333/telemetry 2>/dev/null | python3 -c "import sys, json; data=json.load(sys.stdin); print(sum(endpoint.get('500', {}).get('count', 0) for endpoint in data['result']['requests']['rest']['responses'].values()))" 2>/dev/null || echo "0")
              echo "Recent 500 errors: $ERROR_500_COUNT"

              if [ "$ERROR_500_COUNT" -gt 0 ]; then
                echo "‚ö†Ô∏è  Warning: Qdrant has $ERROR_500_COUNT recent 500 errors"
                echo "Restarting Qdrant to clear error state..."
                docker restart raglite-qdrant
                sleep 5
              fi

              exit 0
            fi

            RETRY_COUNT=$((RETRY_COUNT + 1))
            echo "Waiting for Qdrant... (attempt $RETRY_COUNT/$MAX_RETRIES)"
            sleep 1
          done

          echo "‚ùå Qdrant not available after ${MAX_RETRIES}s"
          exit 1

      - name: Run ground truth accuracy tests
        env:
          QDRANT_HOST: localhost
          QDRANT_PORT: 6333
        run: |
          source ci-venv/bin/activate
          echo "========================================="
          echo "NFR6/NFR7 Validation"
          echo "- Target: ‚â•90% retrieval accuracy (NFR6)"
          echo "- Target: ‚â•95% attribution accuracy (NFR7)"
          echo "========================================="
          pytest tests/integration/test_ac3_ground_truth.py \
                 tests/integration/test_accuracy_validation.py \
                 tests/e2e/test_ground_truth.py \
                 -v \
                 --junitxml=pytest-accuracy-report.xml || true
        continue-on-error: true

      - name: Upload accuracy test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: accuracy-validation-results
          path: pytest-accuracy-report.xml
          retention-days: 30

  # ============================================================================
  # JOB 8: Performance Validation (NFR13)
  # ============================================================================
  test-performance:
    name: "‚ö° NFR: Performance Validation (NFR13)"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 20
    needs: [test-integration]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install -e .
          uv pip install pytest pytest-asyncio pytest-benchmark

      - name: Run performance tests
        env:
          QDRANT_HOST: localhost
          QDRANT_PORT: 6333
        run: |
          source ci-venv/bin/activate
          echo "========================================="
          echo "NFR13 Performance Validation"
          echo "- Target: <5s p50 query response"
          echo "- Target: <15s p95 query response"
          echo "========================================="
          if [ -d "tests/performance" ] && [ -n "$(find tests/performance -name 'test_*.py' -type f)" ]; then
            pytest tests/performance/ \
              -v \
              --junitxml=pytest-performance-report.xml || true
          else
            echo "‚ö†Ô∏è  No performance tests found, skipping..."
          fi
        continue-on-error: true

      - name: Upload performance test results
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: performance-validation-results
          path: pytest-performance-report.xml
          retention-days: 30

  # ============================================================================
  # JOB 9: Test Count Validation
  # ============================================================================
  test-count-validation:
    name: "üî¢ NFR: Test Discovery Validation"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 5
    needs: [test-unit]

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Install uv
        run: curl -LsSf https://astral.sh/uv/install.sh | sh

      - name: Install dependencies with uv
        run: |
          uv venv ci-venv
          source ci-venv/bin/activate
          uv pip install -e .
          uv pip install pytest

      - name: Validate test count
        run: |
          source ci-venv/bin/activate
          echo "========================================="
          echo "Validating test discovery..."
          echo "========================================="

          # Count total tests (including slow)
          EXPECTED_MIN_TESTS=355
          TOTAL_TESTS=$(python -m pytest --collect-only -q tests/ -m "" 2>/dev/null | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")

          # Count fast tests (default - excludes slow)
          FAST_TESTS=$(python -m pytest --collect-only -q tests/ 2>/dev/null | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")

          # Count slow tests
          SLOW_TESTS=$(python -m pytest --collect-only -q tests/ -m "slow" 2>/dev/null | tail -1 | grep -oE '[0-9]+' | head -1 || echo "0")

          echo "Total tests (including slow): $TOTAL_TESTS"
          echo "Fast tests (default):         $FAST_TESTS"
          echo "Slow tests:                   $SLOW_TESTS"
          echo "Expected minimum:             $EXPECTED_MIN_TESTS"
          echo ""

          if [ "$TOTAL_TESTS" -ge "$EXPECTED_MIN_TESTS" ]; then
            echo "‚úÖ Test discovery validated ($TOTAL_TESTS tests, $SLOW_TESTS marked slow)"
          else
            echo "‚ùå Test count too low ($TOTAL_TESTS < $EXPECTED_MIN_TESTS)"
            echo "‚ö†Ô∏è  Some tests may not be discovered by pytest"
            exit 1
          fi

  # ============================================================================
  # DOCUMENTATION & SUMMARY
  # ============================================================================

  # ============================================================================
  # JOB 10: Documentation Validation
  # ============================================================================
  docs-validation:
    name: "üìö Documentation: Validation"
    runs-on: [self-hosted, raglite]
    timeout-minutes: 5

    steps:
      - name: Checkout code
        uses: actions/checkout@v4

      - name: Verify required architecture docs exist
        run: |
          echo "Checking for required documentation files..."
          REQUIRED_DOCS=(
            "docs/architecture/1-introduction-vision.md"
            "docs/architecture/2-executive-summary.md"
            "docs/architecture/6-complete-reference-implementation.md"
            "docs/prd/index.md"
            "CLAUDE.md"
            "README.md"
          )

          MISSING=0
          for doc in "${REQUIRED_DOCS[@]}"; do
            if [ ! -f "$doc" ]; then
              echo "‚ùå Missing: $doc"
              MISSING=$((MISSING + 1))
            else
              echo "‚úÖ Found: $doc"
            fi
          done

          if [ $MISSING -gt 0 ]; then
            echo "‚ö†Ô∏è  $MISSING required documentation file(s) missing"
            exit 1
          else
            echo "‚úÖ All required documentation files present"
          fi

  # ============================================================================
  # JOB 11: Build Summary
  # ============================================================================
  build-summary:
    name: "üìä CI Pipeline: Summary"
    runs-on: [self-hosted, raglite]
    needs: [lint, type-check, security, test-unit, test-integration, test-e2e, test-accuracy, test-performance, test-count-validation, docs-validation]
    if: always()

    steps:
      - name: Check all jobs status
        run: |
          echo "============================================================"
          echo "üìä CI PIPELINE SUMMARY"
          echo "============================================================"
          echo ""
          echo "üìù CODE QUALITY:"
          echo "  Lint & Format:     ${{ needs.lint.result }}"
          echo "  Type Check:        ${{ needs.type-check.result }}"
          echo "  Security Scan:     ${{ needs.security.result }}"
          echo ""
          echo "üß™ TEST SUITES (358 tests total):"
          echo "  Unit Tests:        ${{ needs.test-unit.result }} (~200 tests)"
          echo "  Integration:       ${{ needs.test-integration.result }} (~115 tests)"
          echo "  E2E Tests:         ${{ needs.test-e2e.result }} (~28 tests)"
          echo ""
          echo "‚úÖ NFR VALIDATION:"
          echo "  Accuracy (NFR6/NFR7):  ${{ needs.test-accuracy.result }}"
          echo "  Performance (NFR13):   ${{ needs.test-performance.result }}"
          echo "  Test Discovery:        ${{ needs.test-count-validation.result }}"
          echo ""
          echo "üìö DOCUMENTATION:"
          echo "  Validation:        ${{ needs.docs-validation.result }}"
          echo ""
          echo "============================================================"
          echo ""

          # Critical: Unit tests must pass
          if [[ "${{ needs.test-unit.result }}" == "failure" ]]; then
            echo "‚ùå Unit tests failed - blocking merge"
            exit 1
          fi

          # Critical: Linting must pass
          if [[ "${{ needs.lint.result }}" == "failure" ]]; then
            echo "‚ùå Linting failed - blocking merge"
            exit 1
          fi

          # Critical: Test discovery must pass
          if [[ "${{ needs.test-count-validation.result }}" == "failure" ]]; then
            echo "‚ùå Test discovery validation failed - blocking merge"
            echo "   Some tests may not be discovered by pytest!"
            exit 1
          fi

          # Critical: Documentation must exist
          if [[ "${{ needs.docs-validation.result }}" == "failure" ]]; then
            echo "‚ùå Documentation validation failed - blocking merge"
            exit 1
          fi

          # Warnings for non-critical failures
          if [[ "${{ needs.type-check.result }}" == "failure" ]]; then
            echo "‚ö†Ô∏è  Type checking issues detected (non-blocking)"
          fi

          if [[ "${{ needs.security.result }}" == "failure" ]]; then
            echo "‚ö†Ô∏è  Security scan issues detected (non-blocking)"
          fi

          if [[ "${{ needs.test-integration.result }}" == "failure" ]]; then
            echo "‚ö†Ô∏è  Integration test issues detected (non-blocking)"
          fi

          if [[ "${{ needs.test-e2e.result }}" == "failure" ]]; then
            echo "‚ö†Ô∏è  E2E test issues detected (non-blocking)"
          fi

          if [[ "${{ needs.test-accuracy.result }}" == "failure" ]]; then
            echo "‚ö†Ô∏è  Accuracy validation issues detected (non-blocking)"
            echo "   Review NFR6/NFR7 compliance"
          fi

          if [[ "${{ needs.test-performance.result }}" == "failure" ]]; then
            echo "‚ö†Ô∏è  Performance test issues detected (non-blocking)"
            echo "   Review NFR13 compliance"
          fi

          echo ""
          echo "‚úÖ All critical checks passed - ready to merge"
