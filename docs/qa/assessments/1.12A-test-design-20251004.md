# Test Design: Story 1.12A - Ground Truth Test Set Creation

**Date:** 2025-10-04
**Designer:** Quinn (Test Architect)
**Story ID:** 1.12A
**Story Title:** Ground Truth Test Set Creation

---

## Test Strategy Overview

**Total Test Scenarios:** 15
**Unit Tests:** 11 (73%)
**Integration Tests:** 4 (27%)
**E2E Tests:** 0 (0% - Manual validation process serves as E2E)

**Priority Distribution:**
- **P0:** 8 scenarios (53%) - Data integrity and structural validation
- **P1:** 5 scenarios (33%) - Helper scripts and accessibility
- **P2:** 2 scenarios (14%) - Documentation and metadata quality

**Rationale for Test Distribution:**

This story is fundamentally about **creating test data** rather than application functionality. The testing approach focuses on:
1. **Data structure validation** (Unit) - Ensuring the ground truth data is well-formed, complete, and correctly distributed
2. **Data accessibility** (Integration) - Verifying other modules can import and use the data
3. **Manual validation** (Process) - Human verification that answers exist in source PDFs

**No E2E tests** are needed because this story creates the ground truth that WILL BE USED for E2E accuracy testing in Story 1.12B.

---

## Test Scenarios by Acceptance Criteria

### AC1: Expand Week 0 ground truth (15 queries) to 50+ representative financial queries

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.12A-UNIT-001 | Unit | P0 | Verify GROUND_TRUTH_QA contains at least 50 questions | Critical requirement - data set size validation |
| 1.12A-UNIT-002 | Unit | P0 | Verify all question IDs are unique (no duplicates) | Data integrity - prevents lookup errors |
| 1.12A-UNIT-003 | Unit | P1 | Verify question IDs are sequential (1-50+) | Data organization - aids debugging |

**Coverage Analysis:**
- **Happy Path:** Test data contains 50+ questions with unique, sequential IDs
- **Error Path:** Detect duplicate IDs, gaps in sequence, insufficient total count
- **Edge Cases:** Exact boundary (50 questions), over-provisioning (55+ questions)

**Testing Notes:**
- Use `len(GROUND_TRUTH_QA)` assertion for count
- Build set of IDs and compare length to list length for uniqueness
- Check `sorted(ids) == list(range(1, len(ids) + 1))` for sequence

---

### AC2: Cover all categories (cost_analysis, margins, financial_performance, safety_metrics, workforce, operating_expenses)

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.12A-UNIT-004 | Unit | P0 | Verify all 6 categories are represented in data set | Critical requirement - ensures comprehensive coverage |
| 1.12A-UNIT-005 | Unit | P1 | Verify category distribution matches target percentages | Quality assurance - validates story design (Table in story line 222) |
| 1.12A-UNIT-006 | Unit | P2 | Verify category names match allowed values exactly | Data consistency - prevents typos |

**Coverage Analysis:**
- **Happy Path:** All 6 categories present with correct distribution
- **Error Path:** Missing categories, incorrect percentages, typo in category name
- **Edge Cases:** Slight variance from target distribution (±5% tolerance acceptable)

**Target Distribution Validation:**
```python
expected_distribution = {
    'cost_analysis': 12,        # 24%
    'margins': 8,               # 16%
    'financial_performance': 10, # 20%
    'safety_metrics': 6,        # 12%
    'workforce': 6,             # 12%
    'operating_expenses': 8     # 16%
}
```

**Testing Notes:**
- Use `Counter` from collections to validate category counts
- Allow ±1 question variance from target per category (reasonable tolerance)
- Validate category names against ALLOWED_CATEGORIES constant

---

### AC3: Difficulty distribution: 40% easy, 40% medium, 20% hard

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.12A-UNIT-007 | Unit | P0 | Verify difficulty distribution is 40/40/20 (±5% tolerance) | Critical NFR - ensures balanced test set |
| 1.12A-UNIT-008 | Unit | P2 | Verify difficulty values are exactly "easy", "medium", or "hard" | Data consistency - prevents typos |

**Coverage Analysis:**
- **Happy Path:** 20 easy (40%), 20 medium (40%), 10 hard (20%) out of 50 questions
- **Error Path:** Incorrect distribution, invalid difficulty values
- **Edge Cases:** Boundary tolerance (38%-42% acceptable for easy/medium, 18%-22% for hard)

**Testing Notes:**
- Calculate percentages: `(count / total) * 100`
- Use ±5% tolerance to allow for rounding (e.g., 40% = 38%-42% acceptable)
- If 55 questions instead of 50, adjust percentages accordingly

---

### AC4: Store in `raglite/tests/ground_truth.py` as structured data (JSON or Python dict)

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.12A-INT-001 | Integration | P0 | Verify ground_truth.py can be imported successfully | Critical accessibility - other modules must use this data |
| 1.12A-INT-002 | Integration | P0 | Verify GROUND_TRUTH_QA is exported and is a list type | Critical contract - dependent code expects list of dicts |
| 1.12A-UNIT-009 | Unit | P1 | Verify each question is a dictionary with correct structure | Data type validation - ensures parsability |

**Coverage Analysis:**
- **Happy Path:** Module imports, GROUND_TRUTH_QA is accessible list of dicts
- **Error Path:** Import errors, GROUND_TRUTH_QA not defined, wrong data type
- **Edge Cases:** Empty list (should fail AC1), non-list type (tuple, generator)

**Testing Notes:**
```python
from raglite.tests.ground_truth import GROUND_TRUTH_QA

def test_ground_truth_import():
    assert isinstance(GROUND_TRUTH_QA, list)
    assert len(GROUND_TRUTH_QA) >= 50
    assert all(isinstance(qa, dict) for qa in GROUND_TRUTH_QA)
```

---

### AC5: Each query includes required fields (question, expected_answer, source_document, expected_page_number, expected_section, expected_keywords)

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.12A-UNIT-010 | Unit | P0 | Verify all questions contain ALL required fields | Critical data integrity - missing fields break downstream tests |
| 1.12A-UNIT-011 | Unit | P1 | Verify field types are correct (page_number is int, keywords is list) | Data quality - prevents runtime errors |
| 1.12A-INT-003 | Integration | P1 | Verify helper validation script detects missing fields | Validation tooling - catches errors early |

**Coverage Analysis:**
- **Happy Path:** All questions have all 9 required fields with correct types
- **Error Path:** Missing fields, null/None values, incorrect types
- **Edge Cases:** Empty strings (should fail), empty lists for keywords (acceptable if intentional)

**Required Fields Schema:**
```python
REQUIRED_FIELDS = {
    'id': int,
    'question': str,
    'expected_answer': str,
    'expected_keywords': list,
    'source_document': str,
    'expected_page_number': int,
    'expected_section': str,
    'category': str,
    'difficulty': str
}
```

**Testing Notes:**
- Iterate through all questions and check `field in qa` for each required field
- Validate types using `isinstance(qa[field], expected_type)`
- Ensure no field is None, empty string (except where semantically valid)

---

### AC6: Manual validation - All answers verified against Week 0 test PDF

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| 1.12A-INT-004 | Integration | P1 | Verify validation date is documented in module docstring | Process verification - proves manual validation occurred |

**Coverage Analysis:**
- **Manual Process:** This AC requires human validation against the PDF (not automatable)
- **Automated Check:** Verify documentation includes validation date and process notes
- **Quality Gate:** QA review confirms validation checklist exists or is documented

**Testing Notes:**
- Manual validation is NOT unit/integration testable
- Automated test only verifies **documentation of validation** exists
- QA gate should require evidence: validation date in docstring, checklist file, or story completion notes

---

### AC7: Documentation - README explains ground truth structure and maintenance

#### Scenarios

| ID | Level | Priority | Test Scenario | Justification |
|---|---|---|---|---|
| *Manual Review* | Documentation | P1 | Verify module docstring explains structure, usage, and how to add questions | Documentation quality - aids future maintainers |
| *Manual Review* | Documentation | P2 | Verify difficulty criteria are documented inline | Documentation completeness - guides question creation |

**Coverage Analysis:**
- **Manual Review:** QA verifies docstring contains:
  - Purpose of ground truth test set
  - Usage example (`from raglite.tests.ground_truth import GROUND_TRUTH_QA`)
  - How to add new questions (field descriptions, validation process)
  - Field schema explanation
  - Difficulty rating criteria
- **Automated Check:** Could verify docstring exists and is non-empty (weak validation)

**Testing Notes:**
- Documentation testing is inherently manual (content quality assessment)
- Automated tests can only verify docstring presence, not quality
- QA review gate is the primary validation mechanism

---

## Risk Coverage

**Risk Mitigation Mapping:**

| Risk | Test Scenarios | Mitigation Strategy |
|---|---|---|
| **RISK-001:** Insufficient ground truth size leads to statistical unreliability | 1.12A-UNIT-001 | Validate ≥50 questions programmatically |
| **RISK-002:** Category imbalance causes blind spots in accuracy tracking | 1.12A-UNIT-004, 1.12A-UNIT-005 | Validate all 6 categories and distribution |
| **RISK-003:** Difficulty skew (too easy/too hard) misrepresents system capability | 1.12A-UNIT-007 | Enforce 40/40/20 distribution with tolerance |
| **RISK-004:** Missing required fields break downstream accuracy tests | 1.12A-UNIT-010 | Validate schema completeness for all questions |
| **RISK-005:** Import errors prevent usage by Story 1.12B test runner | 1.12A-INT-001, 1.12A-INT-002 | Test import path and data export |
| **RISK-006:** Incorrect page numbers invalidate attribution testing (NFR7: 95%+ accuracy) | 1.12A-UNIT-011 | Validate page_number is integer type; manual validation required |

---

## Recommended Test Execution Order

### Phase 1: Structural Validation (Fail Fast)
1. **1.12A-INT-001** - Import test (if this fails, nothing else matters)
2. **1.12A-INT-002** - GROUND_TRUTH_QA export validation
3. **1.12A-UNIT-001** - Minimum 50 questions
4. **1.12A-UNIT-002** - Unique IDs

### Phase 2: Data Integrity (P0 Validation)
5. **1.12A-UNIT-004** - All 6 categories present
6. **1.12A-UNIT-007** - Difficulty distribution 40/40/20
7. **1.12A-UNIT-010** - All required fields present

### Phase 3: Data Quality (P1 Validation)
8. **1.12A-UNIT-003** - Sequential IDs
9. **1.12A-UNIT-005** - Category distribution targets
10. **1.12A-UNIT-009** - Dictionary structure validation
11. **1.12A-UNIT-011** - Field type validation
12. **1.12A-INT-003** - Helper validation script

### Phase 4: Documentation & Polish (P2)
13. **1.12A-UNIT-006** - Category name validation
14. **1.12A-UNIT-008** - Difficulty name validation
15. **1.12A-INT-004** - Validation date documented
16. **Manual Review** - Documentation quality

---

## Test Implementation Guidelines

### Unit Test File: `raglite/tests/test_ground_truth.py`

**File Purpose:** Validate ground truth data structure and content

**Recommended Test Structure:**
```python
"""Tests for ground truth test set structure and integrity."""

import pytest
from collections import Counter
from raglite.tests.ground_truth import GROUND_TRUTH_QA

# Test fixtures
REQUIRED_FIELDS = [
    'id', 'question', 'expected_answer', 'expected_keywords',
    'source_document', 'expected_page_number', 'expected_section',
    'category', 'difficulty'
]

ALLOWED_CATEGORIES = [
    'cost_analysis', 'margins', 'financial_performance',
    'safety_metrics', 'workforce', 'operating_expenses'
]

ALLOWED_DIFFICULTIES = ['easy', 'medium', 'hard']


class TestGroundTruthStructure:
    """Structural validation tests (AC1, AC4)."""

    def test_ground_truth_exists_and_is_list(self):
        """1.12A-INT-002: GROUND_TRUTH_QA is exported and is list."""
        assert isinstance(GROUND_TRUTH_QA, list)

    def test_minimum_question_count(self):
        """1.12A-UNIT-001: At least 50 questions present."""
        assert len(GROUND_TRUTH_QA) >= 50

    def test_unique_question_ids(self):
        """1.12A-UNIT-002: All question IDs are unique."""
        ids = [qa['id'] for qa in GROUND_TRUTH_QA]
        assert len(ids) == len(set(ids)), "Duplicate IDs found"

    def test_sequential_question_ids(self):
        """1.12A-UNIT-003: Question IDs are sequential 1-N."""
        ids = sorted([qa['id'] for qa in GROUND_TRUTH_QA])
        expected = list(range(1, len(ids) + 1))
        assert ids == expected


class TestGroundTruthCategories:
    """Category coverage and distribution tests (AC2)."""

    def test_all_categories_present(self):
        """1.12A-UNIT-004: All 6 categories represented."""
        categories = {qa['category'] for qa in GROUND_TRUTH_QA}
        assert categories == set(ALLOWED_CATEGORIES)

    def test_category_distribution(self):
        """1.12A-UNIT-005: Category counts match targets (±1 tolerance)."""
        expected = {
            'cost_analysis': 12,
            'margins': 8,
            'financial_performance': 10,
            'safety_metrics': 6,
            'workforce': 6,
            'operating_expenses': 8
        }
        actual = Counter(qa['category'] for qa in GROUND_TRUTH_QA)

        for category, target in expected.items():
            assert abs(actual[category] - target) <= 1, \
                f"{category}: expected ~{target}, got {actual[category]}"

    def test_category_names_valid(self):
        """1.12A-UNIT-006: Category names match allowed values exactly."""
        for qa in GROUND_TRUTH_QA:
            assert qa['category'] in ALLOWED_CATEGORIES


class TestGroundTruthDifficulty:
    """Difficulty distribution tests (AC3)."""

    def test_difficulty_distribution(self):
        """1.12A-UNIT-007: 40/40/20 distribution (±5% tolerance)."""
        total = len(GROUND_TRUTH_QA)
        difficulties = Counter(qa['difficulty'] for qa in GROUND_TRUTH_QA)

        # Calculate percentages
        easy_pct = (difficulties['easy'] / total) * 100
        medium_pct = (difficulties['medium'] / total) * 100
        hard_pct = (difficulties['hard'] / total) * 100

        # ±5% tolerance
        assert 35 <= easy_pct <= 45, f"Easy: {easy_pct}% (expected 40%)"
        assert 35 <= medium_pct <= 45, f"Medium: {medium_pct}% (expected 40%)"
        assert 15 <= hard_pct <= 25, f"Hard: {hard_pct}% (expected 20%)"

    def test_difficulty_values_valid(self):
        """1.12A-UNIT-008: Difficulty values are valid."""
        for qa in GROUND_TRUTH_QA:
            assert qa['difficulty'] in ALLOWED_DIFFICULTIES


class TestGroundTruthSchema:
    """Field presence and type validation (AC5)."""

    def test_all_required_fields_present(self):
        """1.12A-UNIT-010: All questions contain required fields."""
        for qa in GROUND_TRUTH_QA:
            for field in REQUIRED_FIELDS:
                assert field in qa, f"Missing field '{field}' in question {qa.get('id', 'UNKNOWN')}"

    def test_field_types_correct(self):
        """1.12A-UNIT-011: Field types are correct."""
        for qa in GROUND_TRUTH_QA:
            assert isinstance(qa['id'], int)
            assert isinstance(qa['question'], str)
            assert isinstance(qa['expected_answer'], str)
            assert isinstance(qa['expected_keywords'], list)
            assert isinstance(qa['source_document'], str)
            assert isinstance(qa['expected_page_number'], int)
            assert isinstance(qa['expected_section'], str)
            assert isinstance(qa['category'], str)
            assert isinstance(qa['difficulty'], str)

    def test_each_question_is_dict(self):
        """1.12A-UNIT-009: Each question is a dictionary."""
        assert all(isinstance(qa, dict) for qa in GROUND_TRUTH_QA)


class TestGroundTruthDocumentation:
    """Documentation validation (AC6, AC7)."""

    def test_validation_date_documented(self):
        """1.12A-INT-004: Validation date in module docstring."""
        from raglite.tests import ground_truth
        assert ground_truth.__doc__ is not None
        assert "2025-10-04" in ground_truth.__doc__ or "validation" in ground_truth.__doc__.lower()
```

### Integration Test: Helper Validation Script

**File:** `scripts/validate_ground_truth.py`

**Test:** `raglite/tests/test_validate_script.py`

```python
"""Tests for ground truth validation helper script."""

import subprocess
import pytest


def test_validation_script_detects_missing_fields():
    """1.12A-INT-003: Helper script validates field presence."""
    # This test runs the validation script and checks exit code
    result = subprocess.run(
        ['python', 'scripts/validate_ground_truth.py'],
        capture_output=True,
        text=True
    )

    assert result.returncode == 0, f"Validation failed: {result.stderr}"
    assert "PASS" in result.stdout or "validation complete" in result.stdout.lower()
```

---

## Helper Validation Script Design

**File:** `scripts/validate_ground_truth.py` (Optional but Recommended)

**Purpose:** Standalone script to validate ground truth data integrity

**Functionality:**
- Load `GROUND_TRUTH_QA` from raglite.tests.ground_truth
- Print summary statistics (total, category breakdown, difficulty distribution)
- Validate required fields present
- Check types (page_number is int, etc.)
- Verify distribution targets (40/40/20, category percentages)
- Exit code 0 if all checks pass, 1 if failures detected

**Output Format:**
```
Ground Truth Validation Report
==============================
Total Questions: 50
Categories: ✓ All 6 present
  - cost_analysis: 12 (24%)
  - margins: 8 (16%)
  - financial_performance: 10 (20%)
  - safety_metrics: 6 (12%)
  - workforce: 6 (12%)
  - operating_expenses: 8 (16%)

Difficulty Distribution:
  - easy: 20 (40%) ✓
  - medium: 20 (40%) ✓
  - hard: 10 (20%) ✓

Field Validation: ✓ All required fields present
Type Validation: ✓ All types correct

VALIDATION PASSED ✓
```

---

## Coverage Gaps & Notes

### Coverage Gaps
- **None identified** - All ACs have corresponding test scenarios

### Manual Validation Process
- **AC6** (manual validation against PDF) cannot be automated
- QA gate should require:
  - Validation date documented in ground_truth.py docstring
  - Dev completion notes confirming manual verification
  - Optional: Validation checklist spreadsheet

### Future Enhancements (Not in Scope)
- Automated PDF parsing to verify answers exist on specified pages (Story 1.12B)
- Semantic similarity checks between questions (detect duplicates)
- Keyword extraction validation (verify keywords appear in expected sections)

---

## Quality Checklist

Before marking Story 1.12A complete, verify:

- [x] Every AC has test coverage
- [x] Test levels are appropriate (no E2E over-testing)
- [x] No duplicate coverage across levels
- [x] Priorities align with data integrity risk
- [x] Test IDs follow naming convention (1.12A-LEVEL-SEQ)
- [x] Scenarios are atomic and independent
- [x] P0 tests cover critical data integrity (structure, required fields, distributions)
- [x] Integration tests validate accessibility (import, export)
- [x] Documentation quality verified via manual QA review

---

## Key Principles Applied

✅ **Shift left** - Heavy unit test focus (11 unit vs 4 integration)
✅ **Risk-based** - P0 tests protect against data integrity failures
✅ **Efficient coverage** - No redundant E2E tests for data creation
✅ **Maintainability** - Simple assertion-based tests, easy to update
✅ **Fast feedback** - Unit tests run in milliseconds

---

## Integration with Story 1.12B

**Dependency:** Story 1.12B (Accuracy Test Runner) will:
- Import `GROUND_TRUTH_QA` from this module
- Run queries against ingested documents
- Measure retrieval accuracy (NFR6: 90%+ target)
- Validate source attribution (NFR7: 95%+ target)

**Contract:** This story guarantees:
- `raglite.tests.ground_truth` module is importable
- `GROUND_TRUTH_QA` is exported as list of dicts
- All required fields present and correctly typed
- At least 50 questions with balanced distribution

**Quality Gate:** Story 1.12A cannot be marked DONE until:
- All P0 tests pass
- Manual validation documented
- Helper validation script created (optional but recommended)
- Documentation review completed

---

## Test Summary YAML (for Quality Gate Integration)

```yaml
test_design:
  story_id: "1.12A"
  scenarios_total: 15
  by_level:
    unit: 11
    integration: 4
    e2e: 0
  by_priority:
    p0: 8
    p1: 5
    p2: 2
  coverage_gaps: []
  manual_validation_required: true
  risk_mitigations:
    - RISK-001: Insufficient ground truth size
    - RISK-002: Category imbalance
    - RISK-003: Difficulty skew
    - RISK-004: Missing required fields
    - RISK-005: Import errors
    - RISK-006: Incorrect page numbers
  documentation_review: required
```

---

**End of Test Design Document**
