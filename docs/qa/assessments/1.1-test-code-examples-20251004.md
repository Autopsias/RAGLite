# Test Code Examples: Story 1.1 - Project Setup & Development Environment

**Date:** 2025-10-04
**Author:** Quinn (Test Architect)
**Story:** 1.1 - Project Setup & Development Environment
**Purpose:** Comprehensive test code examples for all test scenarios defined in test design

---

## Table of Contents

1. [Test Infrastructure Setup](#test-infrastructure-setup)
2. [Unit Test Examples](#unit-test-examples)
3. [Integration Test Examples](#integration-test-examples)
4. [E2E Test Examples](#e2e-test-examples)
5. [Test Execution Commands](#test-execution-commands)

---

## Test Infrastructure Setup

### File: `raglite/tests/conftest.py`

```python
"""
Pytest configuration and shared fixtures for RAGLite test suite.

This module provides reusable test fixtures for settings, clients, and test data.
"""

import logging
import os
from pathlib import Path
from typing import Generator

import pytest
from _pytest.monkeypatch import MonkeyPatch
from unittest.mock import MagicMock

from raglite.shared.config import Settings


@pytest.fixture
def test_settings(monkeypatch: MonkeyPatch) -> Settings:
    """
    Provide test settings with safe defaults.

    Args:
        monkeypatch: Pytest monkeypatch fixture for environment variable injection

    Returns:
        Settings instance configured for testing

    Example:
        >>> def test_something(test_settings):
        ...     assert test_settings.qdrant_host == "localhost"
    """
    monkeypatch.setenv("QDRANT_HOST", "localhost")
    monkeypatch.setenv("QDRANT_PORT", "6333")
    monkeypatch.setenv("ANTHROPIC_API_KEY", "test-api-key-12345")
    return Settings()


@pytest.fixture
def test_env_missing_api_key(monkeypatch: MonkeyPatch) -> None:
    """
    Configure environment with missing ANTHROPIC_API_KEY for failure testing.

    Args:
        monkeypatch: Pytest monkeypatch fixture
    """
    monkeypatch.setenv("QDRANT_HOST", "localhost")
    monkeypatch.setenv("QDRANT_PORT", "6333")
    monkeypatch.delenv("ANTHROPIC_API_KEY", raising=False)


@pytest.fixture
def mock_qdrant_client(mocker) -> MagicMock:
    """
    Mock QdrantClient for unit tests.

    Args:
        mocker: pytest-mock fixture

    Returns:
        MagicMock instance configured as QdrantClient

    Example:
        >>> def test_client(mock_qdrant_client):
        ...     mock_qdrant_client.search.return_value = []
        ...     # Use mock in test
    """
    mock_client = mocker.MagicMock()
    mock_client.search.return_value = []
    return mock_client


@pytest.fixture
def mock_claude_client(mocker) -> MagicMock:
    """
    Mock Anthropic Claude client for unit tests.

    Args:
        mocker: pytest-mock fixture

    Returns:
        MagicMock instance configured as Anthropic client
    """
    mock_client = mocker.MagicMock()
    return mock_client


@pytest.fixture
def project_root() -> Path:
    """
    Provide path to project root directory.

    Returns:
        Path object pointing to project root
    """
    # Assumes tests are in raglite/tests/
    return Path(__file__).parent.parent.parent


@pytest.fixture
def temp_env_file(tmp_path: Path) -> Generator[Path, None, None]:
    """
    Create temporary .env file for testing.

    Args:
        tmp_path: Pytest tmp_path fixture

    Yields:
        Path to temporary .env file
    """
    env_file = tmp_path / ".env"
    env_content = """
QDRANT_HOST=localhost
QDRANT_PORT=6333
ANTHROPIC_API_KEY=test-key-abc123
"""
    env_file.write_text(env_content.strip())
    yield env_file
```

---

## Unit Test Examples

### File: `raglite/tests/test_shared_config.py`

```python
"""
Unit tests for raglite.shared.config module.

Tests: 1.1-UNIT-010, 1.1-UNIT-011, 1.1-UNIT-012
"""

import pytest
from pydantic import ValidationError

from raglite.shared.config import Settings


class TestSettingsConfiguration:
    """Test suite for Settings configuration class."""

    def test_settings_load_from_env(self, monkeypatch):
        """
        Test ID: 1.1-UNIT-010
        Verify Settings class loads from .env file correctly.

        Priority: P0
        Level: Unit
        """
        # Arrange
        monkeypatch.setenv("QDRANT_HOST", "test-host")
        monkeypatch.setenv("QDRANT_PORT", "9999")
        monkeypatch.setenv("ANTHROPIC_API_KEY", "sk-ant-test123")

        # Act
        settings = Settings()

        # Assert
        assert settings.qdrant_host == "test-host"
        assert settings.qdrant_port == 9999
        assert settings.anthropic_api_key == "sk-ant-test123"

    def test_settings_missing_anthropic_api_key_raises_error(self, monkeypatch):
        """
        Test ID: 1.1-UNIT-011
        Verify Settings raises error when required ANTHROPIC_API_KEY is missing.

        Priority: P0
        Level: Unit
        Justification: Fail-fast on misconfiguration prevents runtime errors
        """
        # Arrange
        monkeypatch.setenv("QDRANT_HOST", "localhost")
        monkeypatch.setenv("QDRANT_PORT", "6333")
        monkeypatch.delenv("ANTHROPIC_API_KEY", raising=False)

        # Act & Assert
        with pytest.raises(ValidationError) as exc_info:
            Settings()

        assert "anthropic_api_key" in str(exc_info.value).lower()

    def test_settings_uses_default_values_for_optional_fields(self, monkeypatch):
        """
        Test ID: 1.1-UNIT-012
        Verify Settings uses default values for optional variables.

        Priority: P1
        Level: Unit
        """
        # Arrange
        monkeypatch.setenv("ANTHROPIC_API_KEY", "sk-test-key")
        monkeypatch.delenv("QDRANT_HOST", raising=False)
        monkeypatch.delenv("QDRANT_PORT", raising=False)

        # Act
        settings = Settings()

        # Assert
        assert settings.qdrant_host == "localhost"  # Default value
        assert settings.qdrant_port == 6333  # Default value
        assert settings.anthropic_api_key == "sk-test-key"

    def test_settings_loads_from_dotenv_file(self, temp_env_file, monkeypatch):
        """
        Verify Settings loads configuration from .env file.

        Priority: P1
        Level: Unit
        """
        # Arrange
        monkeypatch.setenv("ENV_FILE", str(temp_env_file))

        # Act
        settings = Settings(_env_file=str(temp_env_file))

        # Assert
        assert settings.qdrant_host == "localhost"
        assert settings.qdrant_port == 6333
        assert settings.anthropic_api_key == "test-key-abc123"
```

---

### File: `raglite/tests/test_shared_models.py`

```python
"""
Unit tests for raglite.shared.models module.

Tests: 1.1-UNIT-015, 1.1-UNIT-016, 1.1-UNIT-017
"""

import pytest
from pydantic import ValidationError
from datetime import datetime

from raglite.shared.models import DocumentMetadata, Chunk, SearchResult


class TestDocumentMetadata:
    """Test suite for DocumentMetadata Pydantic model."""

    def test_document_metadata_validates_required_fields(self):
        """
        Test ID: 1.1-UNIT-015
        Verify DocumentMetadata model validates required fields.

        Priority: P0
        Level: Unit
        Justification: Data integrity via Pydantic validation
        """
        # Act
        metadata = DocumentMetadata(
            filename="financial_report_2024.pdf",
            doc_type="PDF",
            ingestion_timestamp="2025-10-04T12:00:00Z"
        )

        # Assert
        assert metadata.filename == "financial_report_2024.pdf"
        assert metadata.doc_type == "PDF"
        assert metadata.ingestion_timestamp == "2025-10-04T12:00:00Z"

    def test_document_metadata_missing_filename_raises_error(self):
        """
        Verify DocumentMetadata raises ValidationError when filename missing.

        Priority: P0
        Level: Unit
        """
        # Act & Assert
        with pytest.raises(ValidationError) as exc_info:
            DocumentMetadata(
                doc_type="PDF",
                ingestion_timestamp="2025-10-04T12:00:00Z"
            )

        assert "filename" in str(exc_info.value).lower()

    def test_document_metadata_field_descriptions_present(self):
        """
        Verify DocumentMetadata fields have Field(...) descriptions.

        Priority: P1
        Level: Unit
        """
        # Arrange & Act
        schema = DocumentMetadata.model_json_schema()

        # Assert
        assert "filename" in schema["properties"]
        assert "description" in schema["properties"]["filename"]
        assert "doc_type" in schema["properties"]
        assert "description" in schema["properties"]["doc_type"]


class TestChunkModel:
    """Test suite for Chunk Pydantic model."""

    def test_chunk_correctly_nests_document_metadata(self):
        """
        Test ID: 1.1-UNIT-016
        Verify Chunk model correctly nests DocumentMetadata.

        Priority: P1
        Level: Unit
        Justification: Model composition validation
        """
        # Arrange
        metadata = DocumentMetadata(
            filename="test.pdf",
            doc_type="PDF",
            ingestion_timestamp="2025-10-04T12:00:00Z"
        )

        # Act
        chunk = Chunk(
            chunk_id="chunk-001",
            content="Revenue increased by 15% in Q4 2024.",
            metadata=metadata
        )

        # Assert
        assert chunk.chunk_id == "chunk-001"
        assert chunk.content == "Revenue increased by 15% in Q4 2024."
        assert isinstance(chunk.metadata, DocumentMetadata)
        assert chunk.metadata.filename == "test.pdf"

    def test_chunk_serialization_includes_nested_metadata(self):
        """
        Verify Chunk serializes with nested metadata correctly.

        Priority: P1
        Level: Unit
        """
        # Arrange
        metadata = DocumentMetadata(
            filename="test.pdf",
            doc_type="PDF",
            ingestion_timestamp="2025-10-04T12:00:00Z"
        )
        chunk = Chunk(
            chunk_id="chunk-001",
            content="Test content",
            metadata=metadata
        )

        # Act
        chunk_dict = chunk.model_dump()

        # Assert
        assert "metadata" in chunk_dict
        assert chunk_dict["metadata"]["filename"] == "test.pdf"
        assert chunk_dict["metadata"]["doc_type"] == "PDF"


class TestSearchResultModel:
    """Test suite for SearchResult Pydantic model."""

    def test_search_result_validates_score_is_float(self):
        """
        Test ID: 1.1-UNIT-017
        Verify SearchResult model validates score is float type.

        Priority: P1
        Level: Unit
        Justification: Type safety validation
        """
        # Arrange
        metadata = DocumentMetadata(
            filename="test.pdf",
            doc_type="PDF",
            ingestion_timestamp="2025-10-04T12:00:00Z"
        )
        chunk = Chunk(
            chunk_id="chunk-001",
            content="Test content",
            metadata=metadata
        )

        # Act
        result = SearchResult(score=0.95, chunk=chunk)

        # Assert
        assert isinstance(result.score, float)
        assert result.score == 0.95

    def test_search_result_rejects_non_float_score(self):
        """
        Verify SearchResult rejects invalid score types.

        Priority: P1
        Level: Unit
        """
        # Arrange
        metadata = DocumentMetadata(
            filename="test.pdf",
            doc_type="PDF",
            ingestion_timestamp="2025-10-04T12:00:00Z"
        )
        chunk = Chunk(
            chunk_id="chunk-001",
            content="Test content",
            metadata=metadata
        )

        # Act & Assert
        with pytest.raises(ValidationError):
            SearchResult(score="invalid", chunk=chunk)
```

---

### File: `raglite/tests/test_shared_clients.py`

```python
"""
Unit tests for raglite.shared.clients module.

Tests: 1.1-UNIT-018, 1.1-UNIT-019, 1.1-UNIT-020
"""

import pytest
from unittest.mock import patch, MagicMock

from raglite.shared.clients import get_qdrant_client, get_claude_client


class TestQdrantClientFactory:
    """Test suite for get_qdrant_client() factory function."""

    @patch('raglite.shared.clients.QdrantClient')
    def test_get_qdrant_client_returns_qdrant_client_instance(self, mock_qdrant_class):
        """
        Test ID: 1.1-UNIT-018
        Verify get_qdrant_client() returns QdrantClient instance.

        Priority: P0
        Level: Unit
        Justification: Factory function contract validation
        """
        # Arrange
        mock_instance = MagicMock()
        mock_qdrant_class.return_value = mock_instance

        # Act
        client = get_qdrant_client()

        # Assert
        assert client == mock_instance
        mock_qdrant_class.assert_called_once()

    @patch('raglite.shared.clients.QdrantClient')
    @patch('raglite.shared.clients.settings')
    def test_get_qdrant_client_uses_settings_configuration(
        self, mock_settings, mock_qdrant_class
    ):
        """
        Test ID: 1.1-UNIT-020
        Verify get_qdrant_client() uses settings.qdrant_host and settings.qdrant_port.

        Priority: P1
        Level: Unit
        Justification: Configuration integration validation
        """
        # Arrange
        mock_settings.qdrant_host = "test-host"
        mock_settings.qdrant_port = 9999
        mock_instance = MagicMock()
        mock_qdrant_class.return_value = mock_instance

        # Act
        client = get_qdrant_client()

        # Assert
        mock_qdrant_class.assert_called_once_with(
            host="test-host",
            port=9999
        )


class TestClaudeClientFactory:
    """Test suite for get_claude_client() factory function."""

    def test_get_claude_client_raises_error_when_api_key_missing(
        self, test_env_missing_api_key
    ):
        """
        Test ID: 1.1-UNIT-019
        Verify get_claude_client() raises error when ANTHROPIC_API_KEY missing.

        Priority: P0
        Level: Unit
        Justification: Security - fail fast on missing credentials
        """
        # Arrange
        # test_env_missing_api_key fixture sets up environment without API key

        # Act & Assert
        with pytest.raises(ValueError) as exc_info:
            get_claude_client()

        assert "ANTHROPIC_API_KEY" in str(exc_info.value)

    @patch('raglite.shared.clients.Anthropic')
    @patch('raglite.shared.clients.settings')
    def test_get_claude_client_returns_anthropic_instance(
        self, mock_settings, mock_anthropic_class
    ):
        """
        Verify get_claude_client() returns Anthropic client instance.

        Priority: P0
        Level: Unit
        """
        # Arrange
        mock_settings.anthropic_api_key = "sk-ant-test123"
        mock_instance = MagicMock()
        mock_anthropic_class.return_value = mock_instance

        # Act
        client = get_claude_client()

        # Assert
        assert client == mock_instance
        mock_anthropic_class.assert_called_once_with(api_key="sk-ant-test123")
```

---

### File: `raglite/tests/test_shared_logging.py`

```python
"""
Unit tests for raglite.shared.logging module.

Tests: 1.1-UNIT-013, 1.1-UNIT-014
"""

import logging
import json
import pytest

from raglite.shared.logging import get_logger


class TestLoggerFactory:
    """Test suite for get_logger() factory function."""

    def test_get_logger_returns_logger_instance(self):
        """
        Test ID: 1.1-UNIT-013
        Verify get_logger() returns logging.Logger instance.

        Priority: P1
        Level: Unit
        Justification: Factory function contract validation
        """
        # Act
        logger = get_logger("test_module")

        # Assert
        assert isinstance(logger, logging.Logger)
        assert logger.name == "test_module"

    def test_logger_outputs_structured_json_format(self, caplog):
        """
        Test ID: 1.1-UNIT-014
        Verify logger outputs structured JSON format with extra={}.

        Priority: P2
        Level: Unit
        Justification: Logging standard compliance
        """
        # Arrange
        logger = get_logger("test_json_logging")

        # Act
        with caplog.at_level(logging.INFO):
            logger.info(
                "Test message",
                extra={"user_id": "123", "action": "test"}
            )

        # Assert
        assert len(caplog.records) == 1
        record = caplog.records[0]
        assert record.user_id == "123"
        assert record.action == "test"

    def test_get_logger_different_names_returns_different_loggers(self):
        """
        Verify get_logger() returns distinct logger instances for different names.

        Priority: P1
        Level: Unit
        """
        # Act
        logger1 = get_logger("module1")
        logger2 = get_logger("module2")

        # Assert
        assert logger1.name == "module1"
        assert logger2.name == "module2"
        assert logger1 != logger2
```

---

### File: `raglite/tests/test_project_structure.py`

```python
"""
Unit tests for project structure validation.

Tests: 1.1-UNIT-002, 1.1-UNIT-003, 1.1-UNIT-004
"""

import pytest
from pathlib import Path


class TestProjectStructure:
    """Test suite for project directory structure validation."""

    def test_raglite_package_root_exists_and_importable(self, project_root):
        """
        Test ID: 1.1-UNIT-002
        Verify raglite/__init__.py exists and is importable as package.

        Priority: P0
        Level: Unit
        Justification: Package root creation blocks all module imports
        """
        # Arrange
        raglite_init = project_root / "raglite" / "__init__.py"

        # Act & Assert
        assert raglite_init.exists(), "raglite/__init__.py does not exist"

        # Test importability
        import raglite
        assert raglite is not None

    def test_all_required_subdirectories_exist(self, project_root):
        """
        Test ID: 1.1-UNIT-003
        Verify all required subdirectories exist.

        Priority: P0
        Level: Unit
        Justification: Directory structure per source-tree.md specification
        """
        # Arrange
        required_dirs = [
            "raglite/shared",
            "raglite/ingestion",
            "raglite/retrieval",
            "raglite/tests",
        ]

        # Act & Assert
        for dir_path in required_dirs:
            full_path = project_root / dir_path
            assert full_path.exists(), f"Required directory {dir_path} does not exist"
            assert full_path.is_dir(), f"{dir_path} exists but is not a directory"

    def test_all_init_files_exist_in_package_directories(self, project_root):
        """
        Test ID: 1.1-UNIT-004
        Verify all __init__.py files exist in package directories.

        Priority: P1
        Level: Unit
        Justification: Python package requirements
        """
        # Arrange
        required_init_files = [
            "raglite/__init__.py",
            "raglite/shared/__init__.py",
            "raglite/ingestion/__init__.py",
            "raglite/retrieval/__init__.py",
            "raglite/tests/__init__.py",
        ]

        # Act & Assert
        for init_file in required_init_files:
            full_path = project_root / init_file
            assert full_path.exists(), f"Required __init__.py file {init_file} does not exist"
```

---

### File: `raglite/tests/test_env_configuration.py`

```python
"""
Unit tests for .env.example configuration validation.

Tests: 1.1-UNIT-005, 1.1-UNIT-006, 1.1-UNIT-007
"""

import pytest
from pathlib import Path


class TestEnvExampleConfiguration:
    """Test suite for .env.example file validation."""

    def test_env_example_contains_required_phase1_variables(self, project_root):
        """
        Test ID: 1.1-UNIT-005
        Verify .env.example contains required Phase 1 variables.

        Priority: P0
        Level: Unit
        Justification: Prevents configuration errors for new developers
        """
        # Arrange
        env_example = project_root / ".env.example"
        required_vars = ["QDRANT_HOST", "QDRANT_PORT", "ANTHROPIC_API_KEY"]

        # Act
        assert env_example.exists(), ".env.example file does not exist"
        content = env_example.read_text()

        # Assert
        for var in required_vars:
            assert var in content, f"Required variable {var} not found in .env.example"

    def test_env_example_has_inline_comments(self, project_root):
        """
        Test ID: 1.1-UNIT-006
        Verify .env.example has inline comments documenting each variable.

        Priority: P1
        Level: Unit
        Justification: Developer experience and onboarding
        """
        # Arrange
        env_example = project_root / ".env.example"

        # Act
        assert env_example.exists(), ".env.example file does not exist"
        content = env_example.read_text()

        # Assert
        # Check for comment character presence
        assert "#" in content, "No comments found in .env.example"

        # Check for documentation-style comments
        lines_with_comments = [line for line in content.split('\n') if '#' in line]
        assert len(lines_with_comments) >= 3, "Insufficient inline comments in .env.example"

    def test_env_example_has_phase4_aws_variables_commented(self, project_root):
        """
        Test ID: 1.1-UNIT-007
        Verify Phase 4 AWS variables are present but commented out.

        Priority: P2
        Level: Unit
        Justification: Forward compatibility without active use
        """
        # Arrange
        env_example = project_root / ".env.example"
        phase4_vars = ["AWS_ACCESS_KEY_ID", "AWS_SECRET_ACCESS_KEY", "AWS_REGION"]

        # Act
        assert env_example.exists(), ".env.example file does not exist"
        content = env_example.read_text()

        # Assert
        for var in phase4_vars:
            # Check variable is present
            assert var in content, f"Phase 4 variable {var} not found in .env.example"

            # Check variable is commented out
            for line in content.split('\n'):
                if var in line:
                    assert line.strip().startswith('#'), f"{var} should be commented out"
```

---

## Integration Test Examples

### File: `raglite/tests/integration/test_docker_qdrant.py`

```python
"""
Integration tests for Docker and Qdrant service validation.

Tests: 1.1-INT-006, 1.1-INT-007, 1.1-INT-008, 1.1-INT-009
"""

import pytest
import subprocess
import requests
import time
from pathlib import Path


class TestDockerQdrantIntegration:
    """Test suite for Docker Compose and Qdrant service validation."""

    @pytest.fixture(scope="class")
    def qdrant_service(self):
        """
        Start Qdrant service via docker-compose for integration tests.

        Yields after service is ready, tears down after tests complete.
        """
        # Start Qdrant
        subprocess.run(
            ["docker-compose", "up", "-d", "qdrant"],
            check=True,
            capture_output=True
        )

        # Wait for Qdrant to be ready
        max_retries = 30
        for i in range(max_retries):
            try:
                response = requests.get("http://localhost:6333/")
                if response.status_code == 200:
                    break
            except requests.ConnectionError:
                pass
            time.sleep(1)

        yield

        # Teardown: stop Qdrant
        subprocess.run(
            ["docker-compose", "down"],
            check=False,
            capture_output=True
        )

    def test_docker_compose_up_starts_qdrant_successfully(self):
        """
        Test ID: 1.1-INT-006
        Verify `docker-compose up -d` starts Qdrant service successfully.

        Priority: P0
        Level: Integration
        Justification: Core infrastructure dependency for all retrieval functionality
        """
        # Act
        result = subprocess.run(
            ["docker-compose", "up", "-d", "qdrant"],
            capture_output=True,
            text=True
        )

        # Assert
        assert result.returncode == 0, f"docker-compose up failed: {result.stderr}"

        # Verify container is running
        ps_result = subprocess.run(
            ["docker-compose", "ps", "--services", "--filter", "status=running"],
            capture_output=True,
            text=True
        )
        assert "qdrant" in ps_result.stdout

    def test_qdrant_health_endpoint_returns_200_ok(self, qdrant_service):
        """
        Test ID: 1.1-INT-007
        Verify Qdrant health endpoint returns 200 OK at http://localhost:6333/.

        Priority: P0
        Level: Integration
        Justification: Validates Qdrant is accessible and operational
        """
        # Act
        response = requests.get("http://localhost:6333/")

        # Assert
        assert response.status_code == 200
        assert response.headers.get("content-type") == "application/json"

    def test_qdrant_version_is_1_11_or_higher(self, qdrant_service):
        """
        Test ID: 1.1-INT-008
        Verify Qdrant version is 1.11.0+ per tech stack requirements.

        Priority: P1
        Level: Integration
        Justification: Version compatibility validation
        """
        # Act
        response = requests.get("http://localhost:6333/")
        data = response.json()

        # Assert
        assert "version" in data
        version_str = data["version"]

        # Parse version (e.g., "1.11.0" -> (1, 11, 0))
        major, minor, patch = map(int, version_str.split('.')[:3])

        assert major >= 1, f"Qdrant major version {major} is less than 1"
        assert minor >= 11, f"Qdrant minor version {minor} is less than 11"

    def test_qdrant_volume_mapping_persists_data(self, qdrant_service, project_root):
        """
        Test ID: 1.1-INT-009
        Verify volume mapping ./qdrant_storage:/qdrant/storage persists data.

        Priority: P1
        Level: Integration
        Justification: Data persistence critical for development workflow
        """
        # Arrange
        storage_path = project_root / "qdrant_storage"

        # Act
        # Qdrant should have created storage directory when started
        time.sleep(2)  # Allow time for directory creation

        # Assert
        assert storage_path.exists(), "qdrant_storage directory not created"
        assert storage_path.is_dir(), "qdrant_storage is not a directory"

        # Check for Qdrant internal files
        storage_contents = list(storage_path.iterdir())
        assert len(storage_contents) > 0, "qdrant_storage directory is empty"
```

---

### File: `raglite/tests/integration/test_uv_dependencies.py`

```python
"""
Integration tests for UV dependency management.

Tests: 1.1-INT-001, 1.1-INT-002, 1.1-INT-003
"""

import pytest
import subprocess
import sys
from pathlib import Path


class TestUVDependencyManagement:
    """Test suite for UV package manager integration."""

    def test_uv_sync_installs_all_dependencies_without_errors(self, project_root):
        """
        Test ID: 1.1-INT-001
        Verify `uv sync --frozen` installs all dependencies without errors.

        Priority: P0
        Level: Integration
        Justification: Critical dependency management - blocks all development
        """
        # Act
        result = subprocess.run(
            ["uv", "sync", "--frozen"],
            cwd=project_root,
            capture_output=True,
            text=True
        )

        # Assert
        assert result.returncode == 0, f"uv sync failed: {result.stderr}"
        assert "error" not in result.stderr.lower()

    def test_installed_package_versions_match_uv_lock(self, project_root):
        """
        Test ID: 1.1-INT-002
        Verify installed package versions match uv.lock file.

        Priority: P0
        Level: Integration
        Justification: Ensures reproducible builds and consistency
        """
        # Arrange
        uv_lock = project_root / "uv.lock"
        assert uv_lock.exists(), "uv.lock file not found"

        # Act
        result = subprocess.run(
            ["uv", "pip", "list", "--format=json"],
            cwd=project_root,
            capture_output=True,
            text=True
        )

        # Assert
        assert result.returncode == 0
        # Basic validation - more detailed version checking would parse JSON
        assert len(result.stdout) > 0, "No packages installed"

    def test_python_version_is_3_11_or_higher(self):
        """
        Test ID: 1.1-INT-003
        Verify Python 3.11+ interpreter is active in virtual environment.

        Priority: P1
        Level: Integration
        Justification: Version requirement per tech stack
        """
        # Act
        version_info = sys.version_info

        # Assert
        assert version_info.major == 3, f"Python major version is {version_info.major}, expected 3"
        assert version_info.minor >= 11, f"Python minor version is {version_info.minor}, expected >=11"
```

---

### File: `raglite/tests/integration/test_precommit_hooks.py`

```python
"""
Integration tests for pre-commit hooks configuration.

Tests: 1.1-INT-011, 1.1-INT-012, 1.1-INT-013
"""

import pytest
import subprocess
from pathlib import Path


class TestPreCommitHooks:
    """Test suite for pre-commit hooks integration."""

    def test_precommit_install_succeeds_without_errors(self, project_root):
        """
        Test ID: 1.1-INT-011
        Verify `uv run pre-commit install` succeeds without errors.

        Priority: P0
        Level: Integration
        Justification: Blocks quality gate automation
        """
        # Act
        result = subprocess.run(
            ["uv", "run", "pre-commit", "install"],
            cwd=project_root,
            capture_output=True,
            text=True
        )

        # Assert
        assert result.returncode == 0, f"pre-commit install failed: {result.stderr}"
        assert "error" not in result.stderr.lower()

    def test_precommit_run_executes_ruff_formatter_and_linter(self, project_root):
        """
        Test ID: 1.1-INT-012
        Verify `uv run pre-commit run --all-files` executes Ruff formatter and linter.

        Priority: P0
        Level: Integration
        Justification: Validates pre-commit hooks are functional
        """
        # Act
        result = subprocess.run(
            ["uv", "run", "pre-commit", "run", "--all-files"],
            cwd=project_root,
            capture_output=True,
            text=True
        )

        # Assert (may fail if code has formatting issues, which is expected)
        assert "ruff" in result.stdout.lower() or "ruff" in result.stderr.lower()
        # Verify both formatter and linter ran
        output = result.stdout + result.stderr
        assert "format" in output.lower() or "check" in output.lower()

    def test_precommit_hooks_reject_commits_with_formatting_violations(
        self, project_root, tmp_path
    ):
        """
        Test ID: 1.1-INT-013
        Verify pre-commit hooks reject commits with formatting violations.

        Priority: P1
        Level: Integration
        Justification: Quality gate enforcement validation
        """
        # Arrange - Create a temporary Python file with formatting violations
        test_file = tmp_path / "bad_format.py"
        test_file.write_text("x=1+2  # Bad formatting\n")

        # Copy to raglite directory
        import shutil
        target_file = project_root / "raglite" / "bad_format.py"
        shutil.copy(test_file, target_file)

        try:
            # Act
            result = subprocess.run(
                ["uv", "run", "pre-commit", "run", "--files", str(target_file)],
                cwd=project_root,
                capture_output=True,
                text=True
            )

            # Assert
            # Pre-commit should fail (non-zero return code) for badly formatted files
            # OR it should auto-fix and indicate changes were made
            assert result.returncode != 0 or "fixed" in result.stdout.lower()

        finally:
            # Cleanup
            if target_file.exists():
                target_file.unlink()
```

---

## E2E Test Examples

### File: `raglite/tests/e2e/test_readme_setup.py`

```python
"""
End-to-end tests for README setup instructions.

Tests: 1.1-E2E-001
"""

import pytest
import subprocess
import time
import requests
from pathlib import Path


class TestReadmeSetupInstructions:
    """Test suite for end-to-end README setup validation."""

    def test_follow_readme_setup_instructions_end_to_end(self, project_root):
        """
        Test ID: 1.1-E2E-001
        Follow README setup instructions end-to-end and verify environment starts successfully.

        Priority: P0
        Level: E2E
        Justification: Critical path for new developer onboarding

        This test simulates a new developer following the README setup instructions
        from scratch and validates the environment starts successfully.
        """
        # Step 1: Verify README.md exists
        readme = project_root / "README.md"
        assert readme.exists(), "README.md not found"

        # Step 2: Run uv sync (simulate: Install dependencies)
        print("\n[E2E] Step 1: Installing dependencies with uv sync...")
        result = subprocess.run(
            ["uv", "sync", "--frozen"],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=120
        )
        assert result.returncode == 0, f"uv sync failed: {result.stderr}"
        print("[E2E] ✓ Dependencies installed")

        # Step 3: Start Docker Compose (simulate: Start Qdrant)
        print("[E2E] Step 2: Starting Qdrant with docker-compose...")
        result = subprocess.run(
            ["docker-compose", "up", "-d"],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=60
        )
        assert result.returncode == 0, f"docker-compose up failed: {result.stderr}"
        print("[E2E] ✓ Qdrant service started")

        try:
            # Step 4: Verify Qdrant health (simulate: Verify services running)
            print("[E2E] Step 3: Waiting for Qdrant to be ready...")
            max_retries = 30
            qdrant_ready = False

            for i in range(max_retries):
                try:
                    response = requests.get("http://localhost:6333/", timeout=2)
                    if response.status_code == 200:
                        qdrant_ready = True
                        break
                except requests.RequestException:
                    pass
                time.sleep(1)

            assert qdrant_ready, "Qdrant failed to become ready within 30 seconds"
            print("[E2E] ✓ Qdrant is ready and responding")

            # Step 5: Run pytest (simulate: Verify test framework)
            print("[E2E] Step 4: Running pytest to verify test framework...")
            result = subprocess.run(
                ["uv", "run", "pytest", "--collect-only"],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=30
            )
            # Pytest should succeed even if no tests exist yet
            assert result.returncode in [0, 5], f"pytest failed: {result.stderr}"
            print("[E2E] ✓ Test framework operational")

            # Step 6: Run ruff format (simulate: Verify code formatting)
            print("[E2E] Step 5: Running ruff format...")
            result = subprocess.run(
                ["uv", "run", "ruff", "format", "raglite/", "--check"],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=30
            )
            # Ruff may fail if formatting needed, which is acceptable
            print("[E2E] ✓ Ruff format executed")

            # Step 7: Run ruff check (simulate: Verify linting)
            print("[E2E] Step 6: Running ruff check...")
            result = subprocess.run(
                ["uv", "run", "ruff", "check", "raglite/"],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=30
            )
            # Ruff may fail if linting issues exist, which is acceptable
            print("[E2E] ✓ Ruff check executed")

            print("\n[E2E] ✅ All README setup instructions validated successfully!")

        finally:
            # Cleanup: Stop Docker Compose
            print("\n[E2E] Cleanup: Stopping Docker services...")
            subprocess.run(
                ["docker-compose", "down"],
                cwd=project_root,
                capture_output=True,
                timeout=30
            )
            print("[E2E] ✓ Cleanup complete")
```

---

### File: `raglite/tests/e2e/test_full_workflow.py`

```python
"""
End-to-end tests for full development workflow.

Tests: 1.1-E2E-002
"""

import pytest
import subprocess
import time
import requests
from pathlib import Path


class TestFullDevelopmentWorkflow:
    """Test suite for complete development workflow validation."""

    def test_full_development_workflow_validation(self, project_root):
        """
        Test ID: 1.1-E2E-002
        Run full validation workflow: uv sync → docker-compose up → pytest → ruff.

        Priority: P0
        Level: E2E
        Justification: Complete development workflow validation

        Workflow Steps:
        1. Execute `uv sync --frozen`
        2. Execute `docker-compose up -d`
        3. Verify Qdrant health check
        4. Execute `uv run pytest` (should discover tests, no failures)
        5. Execute `uv run ruff format raglite/`
        6. Execute `uv run ruff check raglite/`
        7. All steps succeed without errors
        """
        print("\n[E2E WORKFLOW] Starting full development workflow test...")

        # Step 1: uv sync
        print("\n[1/6] Running uv sync --frozen...")
        result = subprocess.run(
            ["uv", "sync", "--frozen"],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=120
        )
        assert result.returncode == 0, f"Step 1 failed - uv sync: {result.stderr}"
        print("✓ Step 1 complete: Dependencies synced")

        # Step 2: docker-compose up
        print("\n[2/6] Running docker-compose up -d...")
        result = subprocess.run(
            ["docker-compose", "up", "-d"],
            cwd=project_root,
            capture_output=True,
            text=True,
            timeout=60
        )
        assert result.returncode == 0, f"Step 2 failed - docker-compose up: {result.stderr}"
        print("✓ Step 2 complete: Docker services started")

        try:
            # Step 3: Qdrant health check
            print("\n[3/6] Verifying Qdrant health check...")
            max_retries = 30
            qdrant_ready = False

            for i in range(max_retries):
                try:
                    response = requests.get("http://localhost:6333/", timeout=2)
                    if response.status_code == 200:
                        qdrant_ready = True
                        print(f"✓ Qdrant ready after {i+1} seconds")
                        break
                except requests.RequestException:
                    pass
                time.sleep(1)

            assert qdrant_ready, "Step 3 failed - Qdrant health check timeout"
            print("✓ Step 3 complete: Qdrant is healthy")

            # Step 4: pytest
            print("\n[4/6] Running uv run pytest...")
            result = subprocess.run(
                ["uv", "run", "pytest", "-v"],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=60
            )
            # pytest returncode 0 = all passed, 5 = no tests collected (acceptable)
            assert result.returncode in [0, 5], f"Step 4 failed - pytest: {result.stderr}"
            print(f"✓ Step 4 complete: pytest (return code: {result.returncode})")

            # Step 5: ruff format
            print("\n[5/6] Running uv run ruff format raglite/...")
            result = subprocess.run(
                ["uv", "run", "ruff", "format", "raglite/"],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=30
            )
            assert result.returncode == 0, f"Step 5 failed - ruff format: {result.stderr}"
            print("✓ Step 5 complete: Code formatted")

            # Step 6: ruff check
            print("\n[6/6] Running uv run ruff check raglite/...")
            result = subprocess.run(
                ["uv", "run", "ruff", "check", "raglite/"],
                cwd=project_root,
                capture_output=True,
                text=True,
                timeout=30
            )
            assert result.returncode == 0, f"Step 6 failed - ruff check: {result.stderr}"
            print("✓ Step 6 complete: Linting passed")

            print("\n" + "="*60)
            print("✅ FULL DEVELOPMENT WORKFLOW VALIDATION SUCCESSFUL")
            print("="*60)

        finally:
            # Cleanup
            print("\n[CLEANUP] Stopping Docker services...")
            subprocess.run(
                ["docker-compose", "down"],
                cwd=project_root,
                capture_output=True,
                timeout=30
            )
            print("✓ Cleanup complete")
```

---

## Test Execution Commands

### Quick Reference

```bash
# Run all tests
uv run pytest

# Run with verbose output
uv run pytest -v

# Run specific test level
uv run pytest raglite/tests/test_*.py           # Unit tests only
uv run pytest raglite/tests/integration/        # Integration tests only
uv run pytest raglite/tests/e2e/                # E2E tests only

# Run specific test file
uv run pytest raglite/tests/test_shared_config.py

# Run specific test
uv run pytest raglite/tests/test_shared_config.py::TestSettingsConfiguration::test_settings_load_from_env

# Run with coverage
uv run pytest --cov=raglite --cov-report=html

# Run tests matching pattern
uv run pytest -k "qdrant"

# Run P0 tests only (requires pytest markers)
uv run pytest -m "p0"

# Run in parallel (requires pytest-xdist)
uv run pytest -n auto
```

### Recommended Execution Order (Fast Feedback)

```bash
# Phase 1: Fast unit tests (P0)
uv run pytest raglite/tests/test_shared_*.py -v

# Phase 2: Integration tests (P0)
uv run pytest raglite/tests/integration/ -v

# Phase 3: E2E tests (P0)
uv run pytest raglite/tests/e2e/ -v

# Phase 4: All remaining tests
uv run pytest -v
```

---

## Pytest Configuration

### File: `pyproject.toml` (add to existing file)

```toml
[tool.pytest.ini_options]
minversion = "7.0"
testpaths = ["raglite/tests"]
python_files = ["test_*.py"]
python_classes = ["Test*"]
python_functions = ["test_*"]
addopts = [
    "-ra",
    "--strict-markers",
    "--strict-config",
    "--showlocals",
]
markers = [
    "p0: Priority 0 critical tests",
    "p1: Priority 1 high tests",
    "p2: Priority 2 medium tests",
    "unit: Unit tests",
    "integration: Integration tests",
    "e2e: End-to-end tests",
]
```

---

## Test Coverage Goals

| Module | Target Coverage | Current Status |
|--------|-----------------|----------------|
| `raglite/shared/config.py` | >90% | TBD |
| `raglite/shared/models.py` | >90% | TBD |
| `raglite/shared/clients.py` | >90% | TBD |
| `raglite/shared/logging.py` | >80% | TBD |
| **Overall shared module** | **>85%** | **TBD** |

---

## Notes

1. **All test code follows Story 1.1 coding standards:**
   - ✅ Type hints on all functions
   - ✅ Google-style docstrings
   - ✅ Structured imports (stdlib, third-party, local)
   - ✅ Specific assertions and error messages

2. **Test isolation:**
   - Unit tests mock external dependencies (Qdrant, Claude API)
   - Integration tests use real services (Docker, Qdrant)
   - E2E tests validate complete workflows

3. **Test data management:**
   - Use `raglite/tests/fixtures/` for test configuration files
   - Use pytest fixtures for reusable test data
   - Clean up test artifacts in teardown

4. **Performance considerations:**
   - Unit tests: <5 seconds total
   - Integration tests: <2 minutes total
   - E2E tests: <5 minutes total

---

**END OF TEST CODE EXAMPLES**
