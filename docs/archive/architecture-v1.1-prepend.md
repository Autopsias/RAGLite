# RAGLite Architecture Document

**Version:** 1.1 (Simplified MVP-First Approach - RECOMMENDED)
**Date:** October 3, 2025
**Status:** Ready for Development
**Recommended Path:** Option A (By The Book)

---

## üìñ How to Read This Document

This architecture document presents **TWO approaches**:

### ‚≠ê v1.1 MONOLITHIC APPROACH (SECTIONS 1-9) - **START HERE FOR MVP**
**Recommended for:** Solo developer, 4-5 week MVP, Claude Code pair programming
- Sections 1-9 below define the simplified monolithic architecture
- 600-800 lines of code, single deployment
- All PRD features delivered, 80% less complexity
- **Use this for Phase 1-3 development**

### üìö v1.0 MICROSERVICES REFERENCE (SECTIONS 10+) - **Future Scaling Guide**
**Use when:** Phase 4+ if you have proven scale problems
- Detailed in later sections (Microservices Architecture, Orchestration, etc.)
- Reference architecture for when you need independent service scaling
- **Don't build this until you have REAL scaling needs**

**For MVP Development: Follow v1.1 (Sections 1-9 below)**

---

## Change Log

| Date | Version | Description | Author |
|------|---------|-------------|--------|
| 2025-10-03 | 1.0 | Initial microservices architecture | Winston (Architect) |
| 2025-10-03 | 1.1 | Added simplified monolithic MVP approach as PRIMARY recommendation | Winston (Architect) |

---

# Part 1: v1.1 MONOLITHIC MVP ARCHITECTURE (RECOMMENDED)

---

## 1. Introduction & Vision

### Purpose

RAGLite is an **AI-powered financial intelligence platform** enabling natural language access to company financial documents through Retrieval-Augmented Generation (RAG).

**v1.1 Architecture Goals:**
- ‚úÖ Achieve 90%+ retrieval accuracy with validated technologies
- ‚úÖ Deliver working MVP in 4-5 weeks (solo developer + Claude Code)
- ‚úÖ Start simple (monolithic), evolve to microservices ONLY if scaling requires
- ‚úÖ Minimize vendor lock-in while using production-proven technologies
- ‚úÖ Support local Docker development ‚Üí AWS deployment path

### Scope (v1.1 MVP)

**In Scope:**
- Single monolithic MCP server (FastMCP)
- PDF ingestion with Docling (97.9% table accuracy)
- Vector search with Qdrant + Fin-E5 embeddings
- LLM synthesis with Claude 3.7 Sonnet
- Source attribution (document, page, section)
- Contextual Retrieval chunking (98.1% accuracy)
- Optional GraphRAG (Phase 2 - only if accuracy <90%)
- Forecasting & Insights (Phase 3)

**Out of Scope (MVP):**
- Custom frontend UI (users interact via MCP clients: Claude Code, Claude Desktop)
- Microservices architecture (deferred to Phase 4 if needed)
- Real-time collaboration (Phase 5+)
- Multi-tenant architecture (single user MVP)

### Architectural Principles

1. **Simplicity First** - Start with simplest solution that meets requirements
2. **Validated Technologies** - Use research-proven technologies (Docling, Fin-E5, etc.)
3. **Phased Complexity** - Add complexity ONLY when simpler approaches fail
4. **Evolution Over Perfection** - Ship functional MVP, iterate based on real usage
5. **AI-Agent Friendly** - Clear patterns for Claude Code to implement

---

## 2. Executive Summary

### v1.1 Architectural Vision

RAGLite v1.1 implements a **simplified monolithic MCP server** that delivers all PRD features with 80% less code than microservices. The architecture prioritizes:

- **Rapid delivery:** 4-5 weeks (vs 8-10 weeks for microservices)
- **Reduced complexity:** 600-800 lines (vs 3000+ lines)
- **Same features:** All functional requirements met
- **Future-proof:** Can evolve to microservices in Phase 4 if proven necessary

**Key Decision: START MONOLITHIC, evolve to microservices ONLY when you have REAL scale problems.**

### Key Architectural Decisions

1. **Monolithic MVP First** ‚≠ê **PRIMARY RECOMMENDATION**
   - Single FastMCP server with modular codebase
   - Direct function calls (no service boundaries)
   - One deployment (app + Qdrant containers)
   - **When to microservices:** Phase 4, IF independent scaling needed

2. **Phased Graph Approach** (Research-Validated)
   - **Phase 1:** Contextual Retrieval (98.1% accuracy, $0.82/100 docs)
   - **Phase 2 (conditional):** GraphRAG IF Phase 1 accuracy <90%
   - **Cost savings:** 99% if GraphRAG unnecessary ($0.82 vs $249/year)

3. **Simplified Orchestration**
   - **Phase 1-2:** Direct function calls (no AWS Strands complexity)
   - **Phase 3 (optional):** Add AWS Strands IF complex multi-agent workflows needed
   - **Default:** Keep it simple with direct calls

4. **Production-Proven Technologies** (Research-Validated)
   - Docling: 97.9% table accuracy (surpasses AWS Textract 88%)
   - Fin-E5: 71.05% financial domain accuracy (+5.6% vs general models)
   - MCP Python SDK: Official, 19k GitHub stars
   - Qdrant: Sub-5s retrieval with HNSW indexing

### Architecture at a Glance (v1.1 Monolithic)

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  MCP Clients (Claude Code, Claude Desktop)              ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                     ‚îÇ Model Context Protocol
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  RAGLite Monolithic Server (FastMCP)                    ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  MCP Tools Layer                                   ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ ingest_financial_document()                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ query_financial_documents()                     ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ forecast_kpi() [Phase 3]                        ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚Ä¢ generate_insights() [Phase 3]                   ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Business Logic Modules (Direct Function Calls)    ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ ingestion/  ‚Üí PDF extraction, chunking, embed  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ retrieval/  ‚Üí Vector search, synthesis         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ forecasting/ [Phase 3]                         ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ insights/    [Phase 3]                         ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îÇ                                                          ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê ‚îÇ
‚îÇ  ‚îÇ  Shared Utilities                                  ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ config.py    ‚Üí Settings, environment vars      ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ logging.py   ‚Üí Structured logging              ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îú‚îÄ models.py    ‚Üí Pydantic data models            ‚îÇ ‚îÇ
‚îÇ  ‚îÇ  ‚îî‚îÄ clients.py   ‚Üí Qdrant, Claude API clients      ‚îÇ ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                       ‚îÇ
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ  Data Layer                                              ‚îÇ
‚îÇ  ‚îú‚îÄ Qdrant (Vector DB) ‚Üí Docker container               ‚îÇ
‚îÇ  ‚îú‚îÄ S3/Local Storage ‚Üí Documents                         ‚îÇ
‚îÇ  ‚îî‚îÄ Neo4j [Phase 2 conditional] ‚Üí Graph (if needed)      ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**Deployment:** 2 Docker containers (app + Qdrant) vs 6 for microservices

---

## 3. Repository Structure (Monolithic)

```
raglite/
‚îú‚îÄ‚îÄ pyproject.toml              # Poetry dependencies
‚îú‚îÄ‚îÄ poetry.lock
‚îú‚îÄ‚îÄ docker-compose.yml          # Qdrant + app containers
‚îú‚îÄ‚îÄ Dockerfile
‚îú‚îÄ‚îÄ .env.example
‚îú‚îÄ‚îÄ .gitignore
‚îú‚îÄ‚îÄ README.md
‚îÇ
‚îú‚îÄ‚îÄ raglite/                    # Main Python package (600-800 lines total)
‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îú‚îÄ‚îÄ main.py                # MCP server entrypoint (~200 lines)
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ ingestion/             # Ingestion module (~150 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ pipeline.py        # Docling + chunking + embedding
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ contextual.py      # Contextual Retrieval chunking
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ retrieval/             # Retrieval module (~150 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ search.py          # Qdrant vector search
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ synthesis.py       # LLM answer synthesis
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ attribution.py     # Source citation
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ forecasting/           # Forecasting module (Phase 3, ~100 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ hybrid.py          # Prophet + LLM adjustment
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ insights/              # Insights module (Phase 3, ~100 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ anomalies.py       # Statistical anomaly detection
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ trends.py          # Trend analysis
‚îÇ   ‚îÇ
‚îÇ   ‚îú‚îÄ‚îÄ shared/                # Shared utilities (~100 lines)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ __init__.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ config.py          # Settings (Pydantic BaseSettings)
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ logging.py         # Structured logging setup
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ models.py          # Pydantic data models
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ clients.py         # Qdrant, Claude API clients
‚îÇ   ‚îÇ
‚îÇ   ‚îî‚îÄ‚îÄ tests/                 # Tests co-located (~200 lines)
‚îÇ       ‚îú‚îÄ‚îÄ test_ingestion.py
‚îÇ       ‚îú‚îÄ‚îÄ test_retrieval.py
‚îÇ       ‚îú‚îÄ‚îÄ test_synthesis.py
‚îÇ       ‚îú‚îÄ‚îÄ ground_truth.py    # Accuracy validation (50+ queries)
‚îÇ       ‚îî‚îÄ‚îÄ fixtures/
‚îÇ           ‚îî‚îÄ‚îÄ sample_financial_report.pdf
‚îÇ
‚îú‚îÄ‚îÄ scripts/
‚îÇ   ‚îú‚îÄ‚îÄ setup-dev.sh           # One-command local setup
‚îÇ   ‚îú‚îÄ‚îÄ init-qdrant.py         # Initialize Qdrant collection
‚îÇ   ‚îî‚îÄ‚îÄ run-accuracy-tests.py  # Ground truth validation
‚îÇ
‚îî‚îÄ‚îÄ docs/
    ‚îú‚îÄ‚îÄ architecture.md         # This document
    ‚îú‚îÄ‚îÄ prd.md                  # Product requirements
    ‚îî‚îÄ‚îÄ front-end-spec.md       # MCP response format spec
```

**Total Files:** ~15 Python files
**Total Lines:** 600-800 lines (vs 3000+ for microservices)
**Complexity:** LOW (direct function calls, single deployment)

---

## 4. Research Findings Summary (Validated Technologies)

All technologies are **research-validated** with quantitative performance data:

### 4.1 Document Processing

**Docling PDF Extraction:**
- ‚úÖ 97.9% table cell accuracy on complex financial PDFs
- ‚úÖ Surpasses AWS Textract (88%) and PyMuPDF (no table models)
- ‚úÖ Cost: <$0.005/page (GPU amortized)

**Fin-E5 Embeddings:**
- ‚úÖ 71.05% NDCG@10 on financial domain retrieval
- ‚úÖ +5.6% improvement over general-purpose models
- ‚úÖ Alternative: Voyage-3-large (74.63% commercial option)

### 4.2 Graph Approach: Contextual Retrieval ‚Üí GraphRAG

**Phase 1: Contextual Retrieval** (VALIDATED)
- **Accuracy:** 96.3-98.1% (vs. 94.3% baseline)
- **Cost:** $0.82 one-time for 100 documents
- **Method:** LLM-generated 50-100 token context per chunk
- **Limitation:** May struggle with complex multi-hop queries

**Phase 2: GraphRAG** (CONDITIONAL - only if Phase 1 <90%)
- **Accuracy:** +12-18% on multi-hop relational queries
- **Cost:** $9 construction + $20/month queries (100 docs, 1000 queries)
- **Decision Gate:** If Contextual Retrieval achieves ‚â•90%, SKIP GraphRAG (99% cost savings)

### 4.3 Orchestration: AWS Strands (Optional for Phase 3)

**Why AWS Strands (if needed):**
- ‚úÖ Multi-LLM support (Claude, GPT-4, Gemini, Llama, local models)
- ‚úÖ No vendor lock-in
- ‚úÖ Production-proven (Amazon Q Developer, AWS Glue)
- ‚úÖ Native MCP support

**v1.1 Decision:** Use direct function calls for Phase 1-2, add Strands in Phase 3 ONLY if complex workflows need multi-agent coordination

### 4.4 MCP Server: FastMCP

- ‚úÖ Official Python SDK (19k GitHub stars)
- ‚úÖ Production-ready Streamable HTTP transport
- ‚úÖ Full protocol support (tools, resources, prompts)
- ‚úÖ ASGI integration (can mount in FastAPI if needed)

### 4.5 Forecasting: Hybrid Approach

- ‚úÖ Statistical core (Prophet/ARIMA) + LLM adjustment
- ‚úÖ ¬±8% forecast error (beats ¬±15% PRD target)
- ‚úÖ Cost: $0.015 per forecast

---

## 5. Technology Stack (Definitive)

| Category | Technology | Version | Purpose | Rationale |
|----------|------------|---------|---------|-----------|
| **PDF Extraction** | Docling | Latest | Extract text/tables from PDFs | 97.9% table accuracy, DocLayNet-based |
| **Excel Processing** | openpyxl + pandas | Latest | Extract tabular data | Standard Python libraries |
| **Embedding Model** | Fin-E5 (finance-adapted) | Latest | Generate semantic vectors | 71.05% financial domain accuracy |
| **Chunking** | Contextual Retrieval | N/A | LLM-generated context per chunk | 98.1% retrieval accuracy |
| **Vector Database** | Qdrant | 1.11+ | Store/search embeddings | HNSW indexing, sub-5s retrieval |
| **Graph Database** | Neo4j | 5.x | Knowledge graph (Phase 2 conditional) | Cypher queries, managed cloud option |
| **MCP Server** | FastMCP (MCP Python SDK) | 1.x | Expose tools via MCP protocol | Official SDK, 19k GitHub stars |
| **LLM (Primary)** | Claude 3.7 Sonnet | via API | Reasoning, analysis, synthesis | State-of-art reasoning, 200K context |
| **Forecasting** | Prophet | 1.1+ | Time-series baseline | Facebook library, seasonal handling |
| **Backend Language** | Python | 3.11+ | All application code | RAG ecosystem standard, async support |
| **API Framework** | FastAPI | 0.115+ (optional) | REST endpoints if needed | High performance, async native |
| **Document Storage** | S3 (cloud) / Local FS (dev) | N/A | Store ingested documents | Scalable, versioning, encryption |
| **Secrets** | AWS Secrets Manager / .env | N/A | API keys, credentials | Secure, rotatable |
| **Containerization** | Docker + Docker Compose | Latest | Local development | Service isolation, reproducible |
| **Cloud Platform** | AWS | N/A | Production deployment (Phase 4) | ECS/Fargate, managed services |
| **IaC** | Terraform | Latest | Infrastructure as Code (Phase 4) | Version-controlled infrastructure |
| **CI/CD** | GitHub Actions | N/A | Testing and deployment | Git-integrated |
| **Monitoring** | CloudWatch + Prometheus | N/A | Performance tracking (Phase 4) | AWS native + open-source |
| **Logging** | Structured JSON | N/A | Application logs, audit trail | CloudWatch-compatible |
| **Testing** | pytest + pytest-asyncio | Latest | Python unit/integration tests | Standard testing stack |

---

## 6. Complete Reference Implementation

### 6.1 MCP Server (raglite/main.py)

**This is the COMPLETE reference implementation.** AI agents should copy these patterns for all modules.

*[See architecture-v1.1-insert.md for the complete 250-line reference implementation]*

**Key file:** `raglite/main.py` (~200 lines)

**Demonstrates:**
- ‚úÖ FastMCP server setup with lifespan management
- ‚úÖ Pydantic model definitions for MCP tools
- ‚úÖ Structured logging with context (`extra={}`)
- ‚úÖ Error handling with specific exceptions
- ‚úÖ Type hints for all functions
- ‚úÖ Google-style docstrings
- ‚úÖ Async patterns for I/O operations

**Pattern Summary:**
```python
# 1. Structured Logging
logger.info("Event", extra={"key": "value"})

# 2. Error Handling
try:
    result = await operation()
except SpecificError as e:
    logger.error("Error", extra={"context": value}, exc_info=True)
    raise ValueError(f"User message: {e}")

# 3. Pydantic Models
class RequestModel(BaseModel):
    field: str = Field(..., description="Description")

# 4. Async Functions
async def my_function(param: str) -> Result:
    return await async_operation()

# 5. Type Hints (Required)
def func(x: str, y: Optional[int] = None) -> Dict[str, Any]:
    pass
```

### 6.2 Coding Standards

**Follow PEP 8 with these requirements:**

1. **Type Hints:** Required for all function signatures
2. **Docstrings:** Google-style for all public functions/classes
3. **Import Organization:** stdlib ‚Üí third-party ‚Üí local
4. **Error Handling:** Specific exceptions with context
5. **Logging:** Structured with `extra={}` context
6. **Constants:** UPPER_CASE naming
7. **Functions:** Verb phrases (e.g., `ingest_document`, not `document`)

**Testing Standards:**
- **Organization:** Co-located in `raglite/tests/`
- **Naming:** `test_<module>_<function>_<scenario>.py`
- **Coverage:** 50%+ for MVP (critical paths), 80%+ for Phase 4
- **Tools:** pytest, pytest-asyncio

**Formatting & Linting:**
```bash
# Before commit
poetry run black raglite/
poetry run isort raglite/
poetry run ruff check raglite/
poetry run mypy raglite/
```

---

## 7. Data Layer

### 7.1 Qdrant (Vector Database)

**Purpose:** Store and search document embeddings

**Schema:**
```python
# Collection: financial_documents
{
    "collection_name": "financial_documents",
    "vector_config": {
        "size": 1024,  # Fin-E5 embedding dimension
        "distance": "Cosine"
    }
}

# Point (Document Chunk):
{
    "id": "uuid-chunk-id",
    "vector": [0.123, -0.456, ...],  # 1024-dim
    "payload": {
        "document_id": "doc-12345",
        "document_name": "Q3_2024_Financial_Report.pdf",
        "chunk_text": "Q3 revenue increased 12%...",
        "chunk_context": "This chunk discusses Q3 2024 revenue...",  # Contextual Retrieval
        "page_number": 3,
        "section": "Revenue Analysis",
        "fiscal_period": "Q3_2024"
    }
}
```

**Deployment:**
- **Local:** Docker container (`qdrant/qdrant:latest`)
- **Production:** Qdrant Cloud (managed) or self-hosted EKS

**Performance:**
- Query latency: <100ms (p50), <500ms (p95)
- Throughput: 100+ queries/sec
- Accuracy: 90%+ retrieval on financial test set

### 7.2 S3 / Local Storage (Documents)

**Structure:**
```
raglite-documents/
‚îú‚îÄ‚îÄ raw/                        # Original PDFs/Excel
‚îÇ   ‚îî‚îÄ‚îÄ Q3_2024_Financial_Report.pdf
‚îú‚îÄ‚îÄ processed/
‚îÇ   ‚îî‚îÄ‚îÄ doc-12345/
‚îÇ       ‚îú‚îÄ‚îÄ metadata.json       # Document metadata
‚îÇ       ‚îú‚îÄ‚îÄ chunks/             # Extracted chunks
‚îÇ       ‚îÇ   ‚îú‚îÄ‚îÄ chunk-001.json
‚îÇ       ‚îÇ   ‚îî‚îÄ‚îÄ chunk-002.json
‚îÇ       ‚îî‚îÄ‚îÄ embeddings/
‚îÇ           ‚îî‚îÄ‚îÄ embeddings.npy  # Numpy array of vectors
‚îî‚îÄ‚îÄ versions/                   # Document version history
    ‚îî‚îÄ‚îÄ doc-12345/
        ‚îú‚îÄ‚îÄ v1_2024-09-01.pdf
        ‚îî‚îÄ‚îÄ v2_2024-10-01.pdf
```

**Deployment:**
- **Local:** `./data/documents/` directory
- **Production:** AWS S3 with versioning + encryption

### 7.3 Neo4j (Graph Database - Phase 2 Conditional)

**‚ö†Ô∏è ONLY IMPLEMENT IF PHASE 1 ACCURACY <90% AND FAILURES ARE RELATIONAL**

**Schema:** (See v1.0 reference sections for detailed Neo4j schema)

---

## 8. Phased Implementation Strategy (v1.1 Simplified)

### Phase 1: Monolithic MVP (Weeks 1-4)

**Goal:** Working Q&A system with 90%+ retrieval accuracy

**Week-by-Week Breakdown:**

**Week 1: Ingestion Pipeline**
- Files to create: `main.py`, `ingestion/pipeline.py`, `config.py`
- Features:
  - ‚úÖ PDF extraction (Docling)
  - ‚úÖ Simple chunking (500 words, 50 overlap)
  - ‚úÖ Fin-E5 embedding generation
  - ‚úÖ Qdrant storage
- Deliverable: `ingest_financial_document()` MCP tool works

**Week 2: Retrieval & Search**
- Files to create: `retrieval/search.py`, `retrieval/attribution.py`
- Features:
  - ‚úÖ Vector similarity search (Qdrant)
  - ‚úÖ Source attribution (document, page, section)
  - ‚úÖ Basic result ranking
- Deliverable: `query_financial_documents()` returns relevant chunks

**Week 3: LLM Synthesis**
- Files to create: `retrieval/synthesis.py`
- Features:
  - ‚úÖ Claude API integration
  - ‚úÖ Answer synthesis from chunks
  - ‚úÖ Citation formatting
  - ‚úÖ OPTIONAL: Contextual Retrieval (upgrade chunking if time permits)
- Deliverable: Natural language answers with source citations

**Week 4: Accuracy Validation**
- Files to create: `tests/ground_truth.py`
- Tasks:
  - ‚úÖ Create ground truth test set (20-50 queries)
  - ‚úÖ Manual accuracy validation
  - ‚úÖ Performance measurement (<10s response)
  - ‚úÖ User testing with real documents
- Deliverable: Validation report + Phase 2 decision

**Success Criteria:**
- ‚úÖ Can ingest 5 financial PDFs successfully (‚â•4/5 succeed)
- ‚úÖ 80%+ of test queries return useful answers (‚â•16/20)
- ‚úÖ Query response time <10 seconds (‚â•8/10 queries)
- ‚úÖ All answers include source citations (20/20)

**Decision Gate (End of Week 4):**

**IF Success Criteria Met:**
- ‚úÖ MVP SUCCESS ‚Üí Proceed to Phase 3 (Forecasting/Insights)
- ‚úÖ SKIP Phase 2 (GraphRAG)

**IF Accuracy <80%:**
- Analyze failures:
  - **IF relational queries** (multi-entity correlations) ‚Üí Consider Phase 2
  - **ELSE retrieval quality** ‚Üí Improve chunking/embeddings ‚Üí Retry

**Technologies:**
- FastMCP, Docling, Fin-E5, Qdrant, Claude 3.7 Sonnet
- Docker Compose (Qdrant + app)
- pytest for basic testing

### Phase 2: GraphRAG (Weeks 5-8) - CONDITIONAL

**‚ö†Ô∏è ONLY IMPLEMENT IF PHASE 1 DECISION GATE TRIGGERS**

*(Detailed in v1.0 reference sections below)*

### Phase 3: Intelligence Features (Weeks 9-12 or 5-8)

**Goal:** Add forecasting and proactive insights

*(Detailed in v1.0 reference sections below)*

### Phase 4: Production Readiness (Weeks 13-16)

**Goal:** AWS deployment, monitoring, performance optimization

*(Detailed in v1.0 reference sections below)*

**Microservices Decision (End of Week 16):**
- Refactor to microservices ONLY IF proven scale problems exist
- See "Evolution Path" section below for migration strategy

---

## 9. Deployment Strategy (Simplified)

### Phase 1-3: Local Docker Compose

**docker-compose.yml:**
```yaml
version: '3.8'

services:
  qdrant:
    image: qdrant/qdrant:latest
    ports:
      - "6333:6333"
      - "6334:6334"
    volumes:
      - ./qdrant_data:/qdrant/storage

  raglite:
    build: .
    ports:
      - "5000:5000"
    volumes:
      - ./data:/data
      - ./.env:/app/.env
    environment:
      - QDRANT_HOST=qdrant
      - CLAUDE_API_KEY=${CLAUDE_API_KEY}
    depends_on:
      - qdrant
```

**Setup:**
```bash
# One-command setup
./scripts/setup-dev.sh

# Starts Qdrant + RAGLite on http://localhost:5000
```

### Phase 4: AWS Production

*(See v1.0 reference sections for detailed AWS deployment with Terraform)*

**Simplified Production:**
- ECS Fargate: 1 task (monolithic container)
- Qdrant Cloud: Managed vector DB
- S3: Document storage
- CloudWatch: Monitoring

**Cost (Monolithic):** ~$115-165/month

---

# Part 2: v1.0 MICROSERVICES REFERENCE (Future Scaling)

**Use this section when you have proven scale problems requiring independent service scaling.**

---

*[The rest of the original architecture.md content follows here, serving as detailed reference for when you need to scale to microservices in Phase 4+]*

