<story-context id="story-context-1.4" v="1.1">
  <metadata>
    <epicId>1</epicId>
    <storyId>1.4</storyId>
    <title>Document Chunking &amp; Semantic Segmentation</title>
    <status>Draft</status>
    <generatedAt>2025-10-12</generatedAt>
    <generator>BMAD Story Context Workflow</generator>
    <sourceStoryPath>/Users/ricardocarvalho/DeveloperFolder/RAGLite/docs/stories/story-1.4.md</sourceStoryPath>
  </metadata>

  <story>
    <asA>system</asA>
    <iWant>to chunk ingested documents using semantic segmentation optimized for financial context</iWant>
    <soThat>retrieval returns relevant, complete information without context fragmentation</soThat>
    <tasks>
      <task id="1.4.1" status="pending">
        <name>Implement Chunking Function</name>
        <acceptanceCriteria>1, 2, 3, 8</acceptanceCriteria>
        <subtasks>
          <subtask>Add chunk_document() function to raglite/ingestion/pipeline.py</subtask>
          <subtask>Implement 500-word chunks with 50-word overlap</subtask>
          <subtask>Respect paragraph boundaries where possible (avoid mid-sentence splits)</subtask>
          <subtask>Calculate page numbers for each chunk using character position estimation</subtask>
          <subtask>Return list of Chunk objects with populated metadata (chunk_id, content, page_number)</subtask>
          <subtask>Follow Story 1.2/1.3 patterns: async/await, type hints, docstrings, structured logging</subtask>
        </subtasks>
      </task>
      <task id="1.4.2" status="pending">
        <name>Table-Aware Chunking</name>
        <acceptanceCriteria>2, 4</acceptanceCriteria>
        <subtasks>
          <subtask>Detect table boundaries in extracted content (Docling markdown format)</subtask>
          <subtask>Keep tables within single chunks (don't split mid-table)</subtask>
          <subtask>If table exceeds chunk size, create dedicated chunk for that table</subtask>
          <subtask>Preserve table metadata (page number, table index) in chunk metadata</subtask>
        </subtasks>
      </task>
      <task id="1.4.3" status="pending">
        <name>Integration with Ingestion Pipeline</name>
        <acceptanceCriteria>3, 8</acceptanceCriteria>
        <subtasks>
          <subtask>Update ingest_pdf() to call chunk_document() after extraction</subtask>
          <subtask>Update extract_excel() to call chunk_document() after extraction</subtask>
          <subtask>Ensure page numbers flow through: extraction → chunking → Chunk objects</subtask>
          <subtask>Update DocumentMetadata to include chunk_count field</subtask>
        </subtasks>
      </task>
      <task id="1.4.4" status="pending">
        <name>Unit Tests</name>
        <acceptanceCriteria>6, 9</acceptanceCriteria>
        <subtasks>
          <subtask>Test: test_chunk_document_basic() - Happy path with 1000-word text</subtask>
          <subtask>Test: test_chunk_overlap() - Verify 50-word overlap between consecutive chunks</subtask>
          <subtask>Test: test_chunk_page_numbers() - CRITICAL: Verify all chunks have page_number != None</subtask>
          <subtask>Test: test_chunk_short_document() - Documents shorter than chunk size</subtask>
          <subtask>Test: test_chunk_table_preservation() - Tables not split mid-row</subtask>
          <subtask>Test: test_chunk_boundary_respect() - No mid-sentence splits</subtask>
          <subtask>Mock document content with known structure for predictable testing</subtask>
        </subtasks>
      </task>
      <task id="1.4.5" status="pending">
        <name>Performance &amp; Quality Validation</name>
        <acceptanceCriteria>5, 7</acceptanceCriteria>
        <subtasks>
          <subtask>Manual validation: Review chunks from Week 0 PDF (160 pages)</subtask>
          <subtask>Check: No mid-sentence splits, logical chunk boundaries</subtask>
          <subtask>Measure chunking time for 160-page PDF (target: &lt;48 seconds for 160 pages)</subtask>
          <subtask>Integration test: End-to-end ingestion → chunking with real PDF</subtask>
          <subtask>Log performance metrics: chunks/second, avg chunk size</subtask>
        </subtasks>
      </task>
    </tasks>
  </story>

  <acceptanceCriteria>
    <criterion id="AC1" priority="high">
      <description>Chunking strategy implemented per Architect's specification (500 words/chunk, 50-word overlap, semantic boundaries)</description>
      <verification>Unit test verifies chunk size and overlap parameters; code review confirms semantic boundary respect</verification>
    </criterion>
    <criterion id="AC2" priority="high">
      <description>Financial context preserved across chunks (tables not split mid-row, sections kept together)</description>
      <verification>Unit test for table preservation; manual review of chunked sample documents</verification>
    </criterion>
    <criterion id="AC3" priority="high">
      <description>Chunk metadata includes source document, page number, section heading where applicable</description>
      <verification>Unit test validates Chunk model fields populated correctly</verification>
    </criterion>
    <criterion id="AC4" priority="medium">
      <description>Chunking handles both narrative text and structured tables appropriately</description>
      <verification>Unit tests cover both text and table chunking scenarios</verification>
    </criterion>
    <criterion id="AC5" priority="medium">
      <description>Chunk quality validated manually on sample documents (no mid-sentence splits, logical boundaries)</description>
      <verification>Manual QA review of chunks from Week 0 PDF</verification>
    </criterion>
    <criterion id="AC6" priority="high">
      <description>Unit tests cover chunking logic with various document structures</description>
      <verification>Test suite includes 6+ unit tests covering edge cases</verification>
    </criterion>
    <criterion id="AC7" priority="medium">
      <description>Performance acceptable for 100-page documents (&lt;30 seconds chunking time)</description>
      <verification>Performance test measures chunking time for 160-page PDF (target: &lt;48s)</verification>
    </criterion>
    <criterion id="AC8" priority="critical">
      <description>CRITICAL - PAGE NUMBER PRESERVATION: Chunk metadata MUST include page_number (validate != None for all chunks)</description>
      <verification>Unit test test_chunk_page_numbers() verifies all chunks have page_number != None</verification>
    </criterion>
    <criterion id="AC9" priority="critical">
      <description>CRITICAL - PAGE NUMBER VALIDATION: Unit test verifies page numbers preserved across chunking pipeline (ingestion → chunking → Qdrant storage)</description>
      <verification>Integration test validates page numbers flow through entire pipeline</verification>
    </criterion>
  </acceptanceCriteria>

  <artifacts>
    <docs>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/docs/prd/epic-1-foundation-accurate-retrieval.md</path>
        <title>Epic 1 PRD - Story 1.4 Requirements</title>
        <section>Story 1.4: Document Chunking &amp; Semantic Segmentation</section>
        <snippet>Lines 182-199: Story definition with enhanced acceptance criteria including page number preservation requirements (AC8, AC9)</snippet>
        <relevance>Source of truth for Story 1.4 requirements and acceptance criteria</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/docs/tech-spec-epic-1.md</path>
        <title>Epic 1 Technical Specification - Chunking Implementation</title>
        <section>chunk_document function specification</section>
        <snippet>Lines 350-396: Complete reference implementation of chunking algorithm with 500-word chunks, 50-word overlap, page number estimation logic</snippet>
        <relevance>Authoritative implementation pattern for chunk_document() function</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/docs/architecture/6-complete-reference-implementation.md</path>
        <title>Architecture Reference Implementation</title>
        <section>Section 6.1 MCP Server Implementation Patterns</section>
        <snippet>Lines 1-20: Demonstrates required patterns - FastMCP setup, Pydantic models, structured logging, error handling, type hints, docstrings, async patterns</snippet>
        <relevance>Coding patterns and best practices to follow for all implementations</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/docs/architecture/coding-standards.md</path>
        <title>RAGLite Coding Standards v1.1</title>
        <section>Complete document</section>
        <snippet>Mandatory patterns: type hints (lines 17-49), Google-style docstrings (lines 53-98), structured logging with extra={} (lines 180-232), Pydantic models (lines 234-273), async/await patterns (lines 277-318), error handling (lines 131-177)</snippet>
        <relevance>Mandatory coding standards for all RAGLite development</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/docs/stories/story-1.2.md</path>
        <title>Story 1.2: PDF Document Ingestion (Completed)</title>
        <section>Dev Notes - Patterns to Reuse</section>
        <snippet>Lines 186-201: Proven patterns from Story 1.2 - same module file (pipeline.py), same error handling, same logging, same data models, same async patterns, mock-based unit tests</snippet>
        <relevance>Established patterns from completed Story 1.2 to follow for consistency</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/docs/stories/story-1.3.md</path>
        <title>Story 1.3: Excel Document Ingestion (Completed)</title>
        <section>Dev Notes - Integration patterns</section>
        <snippet>Lines 186-201: Excel ingestion patterns including multi-sheet handling, numeric formatting preservation, sheet number extraction for citations</snippet>
        <relevance>Established patterns from Story 1.3 for handling structured data</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/CLAUDE.md</path>
        <title>Project Instructions - Anti-Over-Engineering Rules</title>
        <section>CRITICAL DEVELOPMENT CONSTRAINTS</section>
        <snippet>Lines 13-73: RULE 1: KISS (Keep It Simple), RULE 2: Technology Stack LOCKED, RULE 3: No Customization Beyond Standard SDKs, RULE 4: User Approval Required, RULE 5: When In Doubt - check reference implementation</snippet>
        <relevance>Critical constraints to prevent over-engineering in this ~600-800 line MVP</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/CLAUDE.md</path>
        <title>Project Instructions - Repository Structure</title>
        <section>Repository Structure (Target)</section>
        <snippet>Lines 119-149: Shows raglite/ingestion/pipeline.py (~150 lines) in context of overall structure</snippet>
        <relevance>Defines target file locations and line count constraints</relevance>
      </doc>
      <doc>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/pyproject.toml</path>
        <title>Project Dependencies</title>
        <section>Production dependencies and dev dependencies</section>
        <snippet>Lines 31-44: Production dependencies including docling, pandas, openpyxl, pydantic. Lines 55-72: Dev dependencies including pytest, pytest-asyncio, pytest-cov</snippet>
        <relevance>Approved technology stack - no new dependencies should be added</relevance>
      </doc>
    </docs>

    <code>
      <artifact>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/ingestion/pipeline.py</path>
        <kind>module</kind>
        <symbol>ingest_pdf, extract_excel, ingest_document</symbol>
        <lines>1-358</lines>
        <reason>Target file for chunk_document() implementation. Contains existing PDF/Excel ingestion functions that will call chunking. Follow established patterns for async functions, error handling, logging.</reason>
      </artifact>
      <artifact>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/shared/models.py</path>
        <kind>module</kind>
        <symbol>Chunk, DocumentMetadata</symbol>
        <lines>22-33</lines>
        <reason>Chunk model definition with required fields: chunk_id, content, metadata, page_number, embedding. Use this model for chunking function return type.</reason>
      </artifact>
      <artifact>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/shared/logging.py</path>
        <kind>module</kind>
        <symbol>get_logger</symbol>
        <lines>10-37</lines>
        <reason>Structured logging utility - use for all logging in chunking function with extra={} for context fields (doc_id, chunk_count, duration_ms)</reason>
      </artifact>
      <artifact>
        <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/tests/unit/test_ingestion.py</path>
        <kind>test_module</kind>
        <symbol>TestIngestPDF, TestExtractExcel, TestIngestDocument</symbol>
        <lines>1-529</lines>
        <reason>Existing test suite with 19 passing tests. Add chunking tests here following same patterns: pytest.mark.asyncio, mock-based tests, tmp_path fixtures, clear assertions.</reason>
      </artifact>
    </code>

    <dependencies>
      <python>
        <package name="docling" version="2.55.1" reason="PDF extraction with table accuracy - used by ingest_pdf()" />
        <package name="openpyxl" version=">=3.1,&lt;4.0" reason="Excel file parsing - used by extract_excel()" />
        <package name="pandas" version=">=2.0,&lt;3.0" reason="Data manipulation and markdown table formatting - used by extract_excel()" />
        <package name="pydantic" version=">=2.0,&lt;3.0" reason="Data validation for Chunk and DocumentMetadata models" />
        <package name="pytest" version="8.4.2" reason="Test framework for unit tests" />
        <package name="pytest-asyncio" version="1.2.0" reason="Async test support for async def test functions" />
        <package name="pytest-mock" version=">=3.12,&lt;4.0" reason="Mocking support for unit tests" />
      </python>
    </dependencies>
  </artifacts>

  <constraints>
    <constraint type="architecture">
      <rule>Extend existing raglite/ingestion/pipeline.py - do not create new files</rule>
      <rule>Target ~60 lines for chunk_document() function (keep under 100 lines total)</rule>
      <rule>Follow Story 1.2/1.3 patterns exactly: same module, same error handling, same logging style</rule>
    </constraint>
    <constraint type="simplicity">
      <rule>KISS Principle: No abstractions, no custom classes, just a simple async function</rule>
      <rule>Use Python stdlib string operations only - no new dependencies</rule>
      <rule>No custom chunking algorithms - use simple word-based sliding window with overlap</rule>
      <rule>If tempted to create ChunkingStrategy base class or similar → STOP and use simple function</rule>
    </constraint>
    <constraint type="data_model">
      <rule>Use existing Chunk model from raglite/shared/models.py - do not modify or extend</rule>
      <rule>Use existing DocumentMetadata model - may add chunk_count field if needed</rule>
      <rule>Page number estimation: char_pos / (total_chars / page_count)</rule>
      <rule>All Chunk objects MUST have page_number != None (AC8 requirement)</rule>
    </constraint>
    <constraint type="testing">
      <rule>Add tests to existing tests/unit/test_ingestion.py file</rule>
      <rule>Minimum 6 unit tests required (AC6): basic, overlap, page_numbers, short_doc, table_preservation, boundary_respect</rule>
      <rule>Use mock-based tests with pytest fixtures (tmp_path, caplog)</rule>
      <rule>Mark integration tests with @pytest.mark.integration</rule>
      <rule>Test coverage target: 80%+ for chunking function</rule>
    </constraint>
    <constraint type="performance">
      <rule>Target: &lt;48 seconds for 160-page PDF (extrapolated from &lt;30s for 100 pages)</rule>
      <rule>Log performance metrics: chunks/second, avg_chunk_size, duration_ms</rule>
      <rule>Use simple algorithms - optimization deferred to Phase 4</rule>
    </constraint>
    <constraint type="nfr">
      <rule>NFR7 (95%+ Source Attribution): Chunk page_number field is CRITICAL - must be populated</rule>
      <rule>Page number validation in unit test is mandatory (AC8, AC9)</rule>
      <rule>Integration test must verify page numbers flow through entire pipeline</rule>
    </constraint>
  </constraints>

  <interfaces>
    <interface>
      <name>chunk_document</name>
      <kind>async_function</kind>
      <signature>async def chunk_document(full_text: str, doc_metadata: DocumentMetadata, chunk_size: int = 500, overlap: int = 50) -> List[Chunk]</signature>
      <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/ingestion/pipeline.py</path>
      <description>Main chunking function - splits document text into semantic chunks with overlap and page number preservation</description>
      <parameters>
        <param name="full_text" type="str">Complete document text from PDF or Excel extraction</param>
        <param name="doc_metadata" type="DocumentMetadata">Document metadata for provenance</param>
        <param name="chunk_size" type="int" default="500">Target chunk size in words</param>
        <param name="overlap" type="int" default="50">Word overlap between consecutive chunks</param>
      </parameters>
      <returns>List[Chunk] - List of Chunk objects with content, page numbers, and metadata</returns>
      <raises>
        <exception type="ValueError">If chunk_size or overlap parameters are invalid</exception>
      </raises>
      <example>
        <description>Page number estimation algorithm (from Tech Spec lines 350-396)</description>
        <code><![CDATA[
# Calculate estimated chars per page for page number estimation
estimated_chars_per_page = len(full_text) / max(doc_metadata.page_count, 1)

# For each chunk, estimate page number based on character position
idx = 0  # Current word index in document
chunk_index = 0

while idx < len(words):
    chunk_words = words[idx:idx + chunk_size]
    chunk_text = " ".join(chunk_words)

    # Calculate character position of this chunk
    char_pos = len(" ".join(words[:idx]))

    # Estimate page number (1-indexed)
    estimated_page = int(char_pos / estimated_chars_per_page) + 1
    estimated_page = min(estimated_page, doc_metadata.page_count)  # Cap at max pages

    # Create Chunk with page_number populated
    chunk = Chunk(
        chunk_id=f"{doc_metadata.filename}_{chunk_index}",
        content=chunk_text,
        metadata=doc_metadata,
        page_number=estimated_page,  # CRITICAL: Must not be None
        embedding=[]
    )

    idx += (chunk_size - overlap)
    chunk_index += 1
        ]]></code>
        <rationale>This algorithm ensures every chunk has a valid page_number != None (AC8 requirement). Character-based estimation is acceptable for MVP accuracy (NFR7: 95%+ attribution).</rationale>
      </example>
    </interface>
    <interface>
      <name>ingest_pdf</name>
      <kind>async_function</kind>
      <signature>async def ingest_pdf(file_path: str) -> DocumentMetadata</signature>
      <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/ingestion/pipeline.py</path>
      <description>Existing function - will be updated to call chunk_document() after Docling extraction</description>
      <integration>Call chunk_document() after line 180, pass full_text and metadata</integration>
    </interface>
    <interface>
      <name>extract_excel</name>
      <kind>async_function</kind>
      <signature>async def extract_excel(file_path: str) -> DocumentMetadata</signature>
      <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/ingestion/pipeline.py</path>
      <description>Existing function - will be updated to call chunk_document() after openpyxl extraction</description>
      <integration>Call chunk_document() after line 357, pass concatenated sheet markdown and metadata</integration>
    </interface>
    <interface>
      <name>Chunk</name>
      <kind>pydantic_model</kind>
      <signature>class Chunk(BaseModel)</signature>
      <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/shared/models.py</path>
      <description>Data model for document chunks - use this for chunk_document() return type</description>
      <fields>
        <field name="chunk_id" type="str" required="true">Unique chunk identifier (e.g., "filename_0")</field>
        <field name="content" type="str" required="true">Chunk text content</field>
        <field name="metadata" type="DocumentMetadata" required="true">Parent document metadata</field>
        <field name="page_number" type="int" default="0">Page number where chunk appears (CRITICAL: must not be None)</field>
        <field name="embedding" type="list[float]" default="[]">Semantic embedding vector (populated by Story 1.5)</field>
      </fields>
    </interface>
    <interface>
      <name>get_logger</name>
      <kind>function</kind>
      <signature>def get_logger(name: str) -> logging.Logger</signature>
      <path>/Users/ricardocarvalho/DeveloperFolder/RAGLite/raglite/shared/logging.py</path>
      <description>Structured logging utility - use for all logging with extra={} context</description>
      <usage>logger = get_logger(__name__); logger.info("msg", extra={"field": value})</usage>
    </interface>
  </interfaces>

  <tests>
    <standards>
      <standard>Use pytest framework with pytest-asyncio for async test support</standard>
      <standard>Mock external dependencies (Docling, openpyxl) using unittest.mock.patch</standard>
      <standard>Use pytest fixtures for test setup: tmp_path for files, caplog for log capture</standard>
      <standard>Structure tests in classes: TestChunkDocument with multiple test methods</standard>
      <standard>Test naming: test_&lt;function&gt;_&lt;scenario&gt; (e.g., test_chunk_document_basic)</standard>
      <standard>Mark async tests with @pytest.mark.asyncio decorator</standard>
      <standard>Include docstrings explaining what each test validates</standard>
      <standard>Use clear assertions with descriptive error messages</standard>
      <standard>Target 80%+ code coverage for chunking function</standard>
      <standard>Follow patterns from existing test_ingestion.py tests (19 passing tests as reference)</standard>
    </standards>

    <locations>
      <location>/Users/ricardocarvalho/DeveloperFolder/RAGLite/tests/unit/test_ingestion.py</location>
      <location>/Users/ricardocarvalho/DeveloperFolder/RAGLite/tests/integration/test_ingestion_integration.py</location>
    </locations>

    <ideas>
      <idea acceptanceCriteria="AC1, AC6">
        <name>test_chunk_document_basic</name>
        <description>Happy path: 1000-word text chunked into 500-word chunks with 50-word overlap. Verify chunk count, chunk sizes, and overlap between consecutive chunks.</description>
      </idea>
      <idea acceptanceCriteria="AC1, AC6">
        <name>test_chunk_overlap</name>
        <description>Verify 50-word overlap between consecutive chunks. Check that last 50 words of chunk N match first 50 words of chunk N+1.</description>
      </idea>
      <idea acceptanceCriteria="AC3, AC8, AC9">
        <name>test_chunk_page_numbers</name>
        <description>CRITICAL: Verify all chunks have page_number != None. Test with multi-page document, validate page estimation algorithm produces reasonable page numbers.</description>
      </idea>
      <idea acceptanceCriteria="AC1, AC6">
        <name>test_chunk_short_document</name>
        <description>Test document shorter than chunk size (e.g., 200 words). Should return single chunk with correct page number.</description>
      </idea>
      <idea acceptanceCriteria="AC2, AC4, AC6">
        <name>test_chunk_table_preservation</name>
        <description>Test chunking with Docling markdown table in content. Verify table is not split mid-row (stretch goal - may defer to future story).</description>
      </idea>
      <idea acceptanceCriteria="AC5, AC6">
        <name>test_chunk_boundary_respect</name>
        <description>Test that chunks respect paragraph boundaries where possible. Check no mid-sentence splits (stretch goal - may use simple heuristic).</description>
      </idea>
      <idea acceptanceCriteria="AC6">
        <name>test_chunk_empty_document</name>
        <description>Edge case: Empty text input. Should return empty list or raise ValueError with clear message.</description>
      </idea>
      <idea acceptanceCriteria="AC6">
        <name>test_chunk_invalid_parameters</name>
        <description>Test invalid chunk_size or overlap parameters (negative, zero, overlap >= chunk_size). Should raise ValueError.</description>
      </idea>
      <idea acceptanceCriteria="AC7, AC9">
        <name>test_chunk_integration_with_pdf</name>
        <description>Integration test: Ingest real PDF (Week 0 sample), verify chunks are created with page numbers, measure performance (&lt;48s for 160 pages).</description>
      </idea>
      <idea acceptanceCriteria="AC9">
        <name>test_chunk_integration_with_excel</name>
        <description>Integration test: Ingest real Excel file, verify sheet chunks are created with sheet numbers as page numbers.</description>
      </idea>
    </ideas>
  </tests>
</story-context>
